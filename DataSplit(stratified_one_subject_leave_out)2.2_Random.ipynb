{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled90.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPtyQmadbS1Eb+r+KsKlp+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CongenitalGlaucoma_AI_project/blob/main/DataSplit(stratified_one_subject_leave_out)2.2_Random.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data_split for one-subject-leave-out stratified 5-fold crossvalidation**"
      ],
      "metadata": {
        "id": "Dxlpd0AbAWf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZREKDUM5uudx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Leave one subject out cross validation + 5-fold stratified cross validation\n",
        "\n",
        "・1症例を抜き出し、その症例のすべての画像をテスト画像とする\n",
        "・残りの症例の内斜視、外斜視、斜視なし群を、同じ症例が群をまたがないように5分割する。\n",
        "・5分割したデータセットのうち4つをtraining、1つをvalidationとして用いてトレーニングを行い、抜き出した1症例のそれぞれの画像のおける正解率を算出する。これを5回繰り返してcross validationとする。\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TkRaZnYjAjZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c92274-574c-45c8-e76b-a007c407ed06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLeave one subject out cross validation + 5-fold stratified cross validation\\n\\n・1症例を抜き出し、その症例のすべての画像をテスト画像とする\\n・残りの症例の内斜視、外斜視、斜視なし群を、同じ症例が群をまたがないように5分割する。\\n・5分割したデータセットのうち4つをtraining、1つをvalidationとして用いてトレーニングを行い、抜き出した1症例のそれぞれの画像のおける正解率を算出する。これを5回繰り返してcross validationとする。\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nPSM5f-yyQfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319ed771-803e-4595-b0fa-d409f02aa948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "204\n",
            "612\n"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import pickle\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "random_seed = 1 #shuffleのシード\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "\n",
        "gla_ortho_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\"\n",
        "gla_eso_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\gla_eso\"\n",
        "gla_exo_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\gla_exo\"\n",
        "cont_ortho_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\cont_ortho\"\n",
        "cont_eso_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\cont_eso\"\n",
        "cont_exo_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\cont_exo\"\n",
        "\n",
        "result_csv_path = r\"F:\\先天性緑内障\\result.csv\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "gla_ortho_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_d\"\n",
        "gla_eso_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_d__内斜視\"\n",
        "gla_exo_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_d__外斜視\"\n",
        "cont_ortho_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_control\"\n",
        "cont_eso_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_control__内斜視\\内斜視かぶりなし\"\n",
        "cont_exo_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_control__外斜視\\外斜視かぶりなし\"\n",
        "\"\"\"\n",
        "\n",
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_class(path_list, className):\n",
        "    class_list = list(itertools.repeat(className,len(path_list)))\n",
        "    return class_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "\n",
        "gla_ortho_path_list = make_path_list(gla_ortho_path)\n",
        "gla_eso_path_list = make_path_list(gla_eso_path)\n",
        "gla_exo_path_list = make_path_list(gla_exo_path)\n",
        "cont_ortho_path_list = make_path_list(cont_ortho_path)\n",
        "cont_eso_path_list = make_path_list(cont_eso_path)\n",
        "cont_exo_path_list = make_path_list(cont_exo_path)\n",
        "\n",
        "#それぞれの項目（path, classes, ID）をリスト化\n",
        "gla_dataset_path = gla_ortho_path_list + gla_eso_path_list + gla_exo_path_list\n",
        "gla_classes = extract_class(gla_ortho_path_list, \"ortho\") + extract_class(gla_eso_path_list, \"eso\") + extract_class(gla_exo_path_list, \"exo\")\n",
        "gla_id = extract_ids(gla_ortho_path_list) + extract_ids(gla_eso_path_list) + extract_ids(gla_exo_path_list)\n",
        "cont_dataset_path = cont_ortho_path_list + cont_eso_path_list + cont_exo_path_list\n",
        "cont_classes = extract_class(cont_ortho_path_list, \"ortho\") + extract_class(cont_eso_path_list, \"eso\") + extract_class(cont_exo_path_list, \"exo\")\n",
        "cont_id = extract_ids(cont_ortho_path_list) + extract_ids(cont_eso_path_list) + extract_ids(cont_exo_path_list)\n",
        "\n",
        "#convert to Numpy(for use of Scikit-Learn)\n",
        "gla_dataset_path = np.array(gla_dataset_path)\n",
        "gla_classes = np.array(gla_classes)\n",
        "gla_id = np.array(gla_id)\n",
        "cont_dataset_path = np.array(cont_dataset_path)\n",
        "cont_classes = np.array(cont_classes)\n",
        "cont_id = np.array(cont_id)\n",
        "\n",
        "print(len(gla_dataset_path))\n",
        "print(len(cont_dataset_path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "------test_dataset[0]\n",
        "  |\n",
        "  |---train_dataset_gla[0]----0\n",
        "  |                        |--1\n",
        "  |                        |--2\n",
        "  |                        |--3\n",
        "  |                        |--4\n",
        "  |---train_dataset_cont[0]----0\n",
        "  |                         |--1\n",
        "  |                         |--2\n",
        "  |                         |--3\n",
        "  |                         |--4\n",
        "  |---val_dataset_gla[0]----0\n",
        "  |                      |--1\n",
        "  |                      |--2\n",
        "  |                      |--3\n",
        "  |                      |--4\n",
        "  |---val_dataset_cont[0]----0\n",
        "  |                       |--1\n",
        "  |                       |--2\n",
        "  |                       |--3\n",
        "  |                       |--4\n",
        "  |---test_dataset[1]\n",
        "  ...\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cBs5SZf2m7Bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c66e23-ff40-4044-cfc7-3d5d9fba9a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n------test_dataset[0]\\n  |\\n  |---train_dataset_gla[0]----0\\n  |                        |--1\\n  |                        |--2\\n  |                        |--3\\n  |                        |--4\\n  |---train_dataset_cont[0]----0\\n  |                         |--1\\n  |                         |--2\\n  |                         |--3\\n  |                         |--4\\n  |---val_dataset_gla[0]----0\\n  |                      |--1\\n  |                      |--2\\n  |                      |--3\\n  |                      |--4\\n  |---val_dataset_cont[0]----0\\n  |                       |--1\\n  |                       |--2\\n  |                       |--3\\n  |                       |--4\\n  |---test_dataset[1]\\n  ...\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_gla, val_dataset_gla,train_dataset_cont, val_dataset_cont, testset, testset_label = [], [], [], [], [], []\n",
        "\n",
        "#まずglaのデータセットから1人分を抜き出す（LeaveOneGroupOut)\n",
        "# one group leave out 見本\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut\n",
        "# 今回のケースでは、groupがIDに該当\n",
        "logo = LeaveOneGroupOut()\n",
        "logo.get_n_splits(gla_dataset_path, gla_classes, gla_id)\n",
        "logo.get_n_splits(groups=gla_id)  # 'groups' is always required\n",
        "\n",
        "k=0\n",
        "for remain_index, test_index in logo.split(gla_dataset_path, gla_classes, gla_id):\n",
        "    gla_dataset_path_remain, gla_dataset_path_test = gla_dataset_path[remain_index], gla_dataset_path[test_index]\n",
        "    gla_classes_remain, gla_classes_test = gla_classes[remain_index], gla_classes[test_index]\n",
        "    gla_id_remain, gla_id_test = gla_id[remain_index], gla_id[test_index]\n",
        "    #print(gla_dataset_path, gla_dataset_path_test, gla_id_train, gla_id_test)\n",
        "    #print(\"test: \"+gla_id_test[0])\n",
        "    #print(\"TRAIN:\", remain_index, \"TEST:\", test_index)\n",
        "    #print(gla_dataset_path_test[0])\n",
        "    testset.append(gla_dataset_path_test.tolist())\n",
        "    testset_label.append(list(itertools.repeat(1, len(gla_dataset_path_test))))\n",
        "\n",
        "    #抜き出した残りのglaについてStratified group 5-foldをかける\n",
        "    # example of stratified group Kfold　見本\n",
        "    # 今回のケースでは、groupがID、yがclassesに該当\n",
        "\n",
        "    cv = StratifiedGroupKFold(n_splits=5, shuffle = True, random_state = random_seed)\n",
        "    m=0 \n",
        "    train_miniset, val_miniset =  [0 for i in range(0, 5)], [0 for i in range(0, 5)]\n",
        "    for train_idxs, val_idxs in cv.split(gla_dataset_path_remain, gla_classes_remain, gla_id_remain):\n",
        "        #print(\"TRAIN:\", gla_classes_remain[train_idxs])\n",
        "        #print(\"      \", gla_id_remain[train_idxs])\n",
        "        #print(\"      \", gla_dataset_path_remain[train_idxs])\n",
        "        #print(\" TEST:\", gla_classes_remain[val_idxs])\n",
        "        #print(\"      \", gla_id_remain[val_idxs])\n",
        "        #print(\"      \", gla_dataset_path_remain[val_idxs])\n",
        "        train_miniset[m] = gla_dataset_path_remain[train_idxs].tolist()\n",
        "        val_miniset[m] = gla_dataset_path_remain[val_idxs].tolist()\n",
        "        m+=1\n",
        "    train_dataset_gla.append(train_miniset)\n",
        "    val_dataset_gla.append(val_miniset)\n",
        "    #print(\"train_dataset_added label[gla] \" + str(k))\n",
        "    k+=1\n",
        "\n",
        "    #control全体についてStratified group 5-foldをかける\n",
        "    m=0 \n",
        "    train_miniset, val_miniset =  [0 for i in range(0, 5)], [0 for i in range(0, 5)]\n",
        "    for train_idxs, val_idxs in cv.split(cont_dataset_path, cont_classes, cont_id):\n",
        "        #print(\"TRAIN:\", cont_classes[train_idxs])\n",
        "        #print(\"      \", cont_id[train_idxs])\n",
        "        #print(\"      \", cont_dataset_path[train_idxs])\n",
        "        #print(\" TEST:\", cont_classes[val_idxs])\n",
        "        #print(\"      \", cont_id[val_idxs])\n",
        "        #print(\"      \", cont_dataset_path[val_idxs])\n",
        "        train_miniset[m] = cont_dataset_path[train_idxs].tolist()\n",
        "        val_miniset[m] = cont_dataset_path[val_idxs].tolist()\n",
        "        m+=1\n",
        "    train_dataset_cont.append(train_miniset)\n",
        "    val_dataset_cont.append(val_miniset)\n",
        "\n",
        "        \n",
        "#print(len(train_dataset_gla))    \n",
        "#print(len(val_dataset_gla))\n",
        "#print(val_dataset_gla)\n",
        "#print(len(train_dataset_cont))    \n",
        "#print(len(val_dataset_cont))\n",
        "#print(len(test_dataset))\n",
        "\n",
        "\n",
        "#同じくcontのデータセットから1人分抜き出してLeaveOneGroupOutをする\n",
        "logo = LeaveOneGroupOut()\n",
        "logo.get_n_splits(cont_dataset_path, cont_classes, cont_id)\n",
        "logo.get_n_splits(groups=cont_id)  # 'groups' is always required\n",
        "\n",
        "k=0\n",
        "for remain_index, test_index in logo.split(cont_dataset_path, cont_classes, cont_id):\n",
        "    cont_dataset_path_remain, cont_dataset_path_test = cont_dataset_path[remain_index], cont_dataset_path[test_index]\n",
        "    cont_classes_remain, cont_classes_test = cont_classes[remain_index], cont_classes[test_index]\n",
        "    cont_id_remain, cont_id_test = cont_id[remain_index], cont_id[test_index]\n",
        "    #print(cont_dataset_path_test[0])\n",
        "    testset.append(cont_dataset_path_test.tolist())\n",
        "    testset_label.append(list(itertools.repeat(0, len(cont_dataset_path_test))))\n",
        "\n",
        "    #抜き出した残りのcontについてStratified group 5-foldをかける\n",
        "\n",
        "    cv = StratifiedGroupKFold(n_splits=5, shuffle = True, random_state = random_seed)\n",
        "    m=0 \n",
        "    train_miniset, val_miniset =  [0 for i in range(0, 5)], [0 for i in range(0, 5)]\n",
        "    for train_idxs, val_idxs in cv.split(cont_dataset_path_remain, cont_classes_remain, cont_id_remain):\n",
        "        train_miniset[m] = cont_dataset_path_remain[train_idxs].tolist()\n",
        "        val_miniset[m] = cont_dataset_path_remain[val_idxs].tolist()\n",
        "        m+=1\n",
        "    train_dataset_cont.append(train_miniset)\n",
        "    val_dataset_cont.append(val_miniset)\n",
        "\n",
        "    #gla全体についてStratified group 5-foldをかける\n",
        "    m=0 \n",
        "    train_miniset, val_miniset =  [0 for i in range(0, 5)], [0 for i in range(0, 5)]\n",
        "    for train_idxs, val_idxs in cv.split(gla_dataset_path, gla_classes, gla_id):\n",
        "        train_miniset[m] = gla_dataset_path[train_idxs].tolist()\n",
        "        val_miniset[m] = gla_dataset_path[val_idxs].tolist()\n",
        "        m+=1\n",
        "    train_dataset_gla.append(train_miniset)\n",
        "    val_dataset_gla.append(val_miniset)\n",
        "    #print(\"train_dataset_added label[cont] \"+ str(k))\n",
        "    k+=1\n",
        "        \n",
        "print(len(train_dataset_gla))    \n",
        "print(len(val_dataset_gla))\n",
        "print(len(train_dataset_cont))    \n",
        "print(len(val_dataset_cont))\n",
        "print(len(testset))\n",
        "print(len(testset_label))\n"
      ],
      "metadata": {
        "id": "XLe8wZOkQkkk",
        "outputId": "76986350-1813-427e-e13f-ac6c36fe8b0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "641\n",
            "641\n",
            "641\n",
            "641\n",
            "641\n",
            "641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modules**"
      ],
      "metadata": {
        "id": "Ef0A_-M7wfS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])      \n",
        "        return tensor_image, target\n",
        "\n",
        "#画像読み込み時間削減のため、Expand2squareの処理は行っている\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "        \n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            \n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            \"\"\"\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \"\"\"\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "        #print()   \n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "           \n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        #####################\n",
        "        # test the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, test_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            \n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        test_acc = running_corrects.item()/len(test_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(num_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}' +'\\n'\n",
        "                     f'test_acc: {test_acc:.5f}' + f'({running_corrects:.0f}/{len(test_dataset):.0f})') \n",
        "\n",
        "        \n",
        "        print(print_msg)\n",
        "        \n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)"
      ],
      "metadata": {
        "id": "NecL-52PwEgB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Define RepVGG**"
      ],
      "metadata": {
        "id": "HLtaBYEKwoYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "\n",
        "#RepVGGのpretrained modelをダウンロード\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "file_id = '1PvtYTOX4gd-1VHX8LoT7s6KIyfTKOf8G'\n",
        "destination = \"F:\\先天性緑内障\\RepVGG-A2.pth\"\n",
        "\n",
        "if os.path.exists(destination) is not True:\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "else:\n",
        "    print(\"pretrained repVGG model already exists\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJpr0PhcwmVo",
        "outputId": "ebb48344-cc05-41e3-b3ff-8315215d9d60"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pretrained repVGG model already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Deploy RepVGG_A2**"
      ],
      "metadata": {
        "id": "Adbsar4tW1Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#deploy RepVGG-A2\n",
        "\"\"\"\n",
        "train_model = create_RepVGG_A2(deploy=False)\n",
        "train_model.load_state_dict(torch.load('/content/drive/MyDrive/Deep_learning/RepVGG-A2-train.pth'))   \n",
        "model_ft = repvgg_model_convert(train_model, create_RepVGG_A2, save_path='/content/drive/MyDrive/Deep_learning/repvgg-A2-deploy.pth')\n",
        "\"\"\"\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "\n",
        "#use pretrained model\n",
        "#model_ft.load_state_dict(torch.load('/content/drive/MyDrive/Deep_learning/RepVGG-A2-train.pth'))   \n",
        "model_ft.load_state_dict(torch.load (\"F:\\先天性緑内障\\RepVGG-A2.pth\"))   \n",
        "num_ftrs = model_ft.linear.in_features\n",
        "model_ft.linear = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))"
      ],
      "metadata": {
        "id": "OwU7cOcVxjLJ",
        "outputId": "b2ca6697-7188-4a95-a0ea-0b9b4d41b859",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dataset and Dataloader**"
      ],
      "metadata": {
        "id": "zi3zgoQ-535o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#フォルダから落としてくるときはこちら\n",
        "\n",
        "train_list = []\n",
        "train_list_label = []\n",
        "val_list = []\n",
        "val_list_label = []\n",
        "test_list = []\n",
        "test_list_label = []\n",
        "\n",
        "#早川さんのデータセット\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\data9卒論用\\train\\dise\\*\"):\n",
        "    train_list.append(img_path)\n",
        "    train_list_label.append(1)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\data9卒論用__square\\train\\cont\\*\"):\n",
        "    train_list.append(img_path)\n",
        "    train_list_label.append(0)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\data9卒論用__square\\val\\dise\\*\"):\n",
        "    val_list.append(img_path)\n",
        "    val_list_label.append(1)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\data9卒論用__square\\val\\cont\\*\"):\n",
        "    val_list.append(img_path)\n",
        "    val_list_label.append(0)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\data9卒論用__square\\test\\dise\\*\"):\n",
        "    test_list.append(img_path)\n",
        "    test_list_label.append(1)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\data9卒論用__square\\test\\cont\\*\"):\n",
        "    test_list.append(img_path)\n",
        "    test_list_label.append(0)\n",
        "\n",
        "#Stratified dataset folder\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\Stratified\\train\\gla\\*\"):\n",
        "    train_list.append(img_path)\n",
        "    train_list_label.append(1)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\Stratified\\train\\cont\\*\"):\n",
        "    train_list.append(img_path)\n",
        "    train_list_label.append(0)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\Stratified\\val\\gla\\*\"):\n",
        "    val_list.append(img_path)\n",
        "    val_list_label.append(1)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\Stratified\\val\\cont\\*\"):\n",
        "    val_list.append(img_path)\n",
        "    val_list_label.append(0)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\data9卒論用__square\\test\\dise\\*\"):\n",
        "    test_list.append(img_path)\n",
        "    test_list_label.append(1)\n",
        "for img_path in glob.glob(r\"F:\\先天性緑内障\\データ引継ぎ\\data9卒論用__square\\test\\cont\\*\"):\n",
        "    test_list.append(img_path)\n",
        "    test_list_label.append(0)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yfbBYBE5uXa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt=4\n",
        "fold=2\n",
        "train_list = train_dataset_gla[pt][fold] + train_dataset_cont[pt][fold]\n",
        "train_list_label = list(itertools.repeat(1, len(train_dataset_gla[pt][fold])))+list(itertools.repeat(0, len(train_dataset_cont[pt][fold])))\n",
        "val_list = val_dataset_gla[pt][fold] + val_dataset_cont[pt][fold]\n",
        "val_list_label = list(itertools.repeat(1, len(val_dataset_gla[pt][fold])))+list(itertools.repeat(0, len(val_dataset_cont[pt][fold])))\n",
        "test_list = testset[pt]\n",
        "test_list_label = testset_label[pt]\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))\n",
        "print(len(test_list))\n",
        "\n",
        "#データセットの確認\n",
        "for i, j in zip(test_list, test_list_label):\n",
        "    print(i,j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwCiliQK7As_",
        "outputId": "5484f8b3-1fdc-4db4-8f7e-c42a52f086f5"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "636\n",
            "175\n",
            "5\n",
            "F:\\先天性緑内障\\dataset_for_article_250px\\gla_exo\\3229_1.jpg 1\n",
            "F:\\先天性緑内障\\dataset_for_article_250px\\gla_exo\\3229_3.jpg 1\n",
            "F:\\先天性緑内障\\dataset_for_article_250px\\gla_exo\\3229_4.jpg 1\n",
            "F:\\先天性緑内障\\dataset_for_article_250px\\gla_exo\\3229_5.jpg 1\n",
            "F:\\先天性緑内障\\dataset_for_article_250px\\gla_exo\\3229_9.jpg 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))\n",
        "print(len(test_list))\n",
        "\n",
        "\"\"\"\n",
        "#データセットの確認\n",
        "for i, j in zip(train_list, train_list_label):\n",
        "    print(i,j)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8BS-eazV7FRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7eeb5f0-6857-4e03-8262-45836765b8e4"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "636\n",
            "175\n",
            "5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#データセットの確認\\nfor i, j in zip(train_list, train_list_label):\\n    print(i,j)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "test_dataset = SimpleImageDataset(test_list, test_list_label, test_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle = False)\n",
        "\n",
        "# Make a grid from batch\n",
        "inputs, classes = next(iter(test_loader))\n",
        "print(classes)\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "class_names = [\"cont\", \"gla\"]\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "metadata": {
        "id": "2pwoEIRBjGEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training and evaluation**"
      ],
      "metadata": {
        "id": "cLaukgf2XfAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model_ft = torchvision.models.resnet50(pretrained=True)  \n",
        "#num_ftrs = model_ft.fc.in_features\n",
        "#model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_ft = create_RepVGG_A2(deploy=False)\n",
        "model_ft.load_state_dict(torch.load (\"F:\\先天性緑内障\\RepVGG-A2.pth\"))   \n",
        "num_ftrs = model_ft.linear.in_features\n",
        "model_ft.linear = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=10, num_epochs=30)"
      ],
      "metadata": {
        "id": "CLT-Dhv-XiMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft.eval() # prep model for evaluation\n",
        "\n",
        "targets, preds =[], []\n",
        "for image_tensor, target in test_loader:  \n",
        "      #target = target.squeeze(1)     \n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "      _, pred = torch.max(output, 1)  \n",
        "      preds.append(int(pred))  #予測結果\n",
        "      targets.append(int(target)) #ラベル\n",
        "\n",
        "y_test = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "print(\"label: \", y_test)\n",
        "print(\"pred: \", y_pred)"
      ],
      "metadata": {
        "id": "hDpz1ontYFbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee89f4e-d4c7-44a8-b395-40ed2653288b"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label:  [1 1 1 1 1]\n",
            "pred:  [1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GradCAM (診断根拠の可視化)\n"
      ],
      "metadata": {
        "id": "uQaq2zGvr1bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "def GradCAM(img, c, features_fn, classifier_fn):\n",
        "    feats = features_fn(img.cuda())\n",
        "    _, N, H, W = feats.size() #[1,2048,7,7]\n",
        "    out = classifier_fn(feats) #out: [1,1000]\n",
        "    c_score = out[0, c]   #c_scoreとは？？\n",
        "\n",
        "    grads = torch.autograd.grad(c_score, feats)\n",
        "    w = grads[0][0].mean(-1).mean(-1)           #ここでGlobalAveragePoolingをしている\n",
        "    sal = torch.matmul(w, feats.view(N, H*W))\n",
        "    sal = sal.view(H, W).cpu().detach().numpy()\n",
        "    sal = np.maximum(sal, 0) #ReLUと同じ\n",
        "    return sal\n",
        "\n",
        "read_tensor = transforms.Compose([\n",
        "    lambda x: Image.open(x),\n",
        "    lambda x: x.convert('RGB'),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    lambda x: torch.unsqueeze(x, 0) #次元を1に引き延ばす\n",
        "])\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      #print('Image: '+ image_name)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      #print('Label: '+ label)\n",
        "      return(image_name, label)\n",
        "\n"
      ],
      "metadata": {
        "id": "-bg9YYJFr6ia"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split model in two parts\n",
        "features_fn = nn.Sequential(*list(model_ft.children())[:-2]) #最後の2層（AdaptiveAvgPool2dとLinear)を取り除いたもの\n",
        "classifier_fn = nn.Sequential(*(list(model_ft.children())[-2:-1] + [Flatten()] + list(model_ft.children())[-1:])) #最終層の前にFlatten()を挿入\n",
        " #最後の2層\n",
        "\n",
        "#評価モードにする    \n",
        "model_ft = model_ft.eval()\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "\n",
        "classes = [\"cont\", \"gla\"]\n",
        "\n",
        "#画像のパスを指定\n",
        "#for j in range(1):\n",
        "for j in range(len(test_dataset)):\n",
        "\n",
        "    #元画像\n",
        "\n",
        "    image = test_dataset[j][0]\n",
        "    image = image.permute(1, 2, 0)\n",
        "\n",
        "    img_tensor = test_dataset[j][0].unsqueeze(0)\n",
        "    #Softmaxにかけたときの確率上位1つのpp(確率)とcc(class番号)を取得(tench→正常,goldfish→斜視)\n",
        "    pp, cc = torch.topk(nn.Softmax(dim=1)(model_ft(img_tensor.to(device))), 1)\n",
        "\n",
        "    #pとcを対にして入力\n",
        "    for i, (p, c) in enumerate(zip(pp[0], cc[0])):  \n",
        "        sal = GradCAM(img_tensor, int(c), features_fn, classifier_fn)\n",
        "        tmp = image.to('cpu').detach().numpy().copy()\n",
        "        img = Image.fromarray((tmp*255).astype(np.uint8))\n",
        "        #TensorをImageに変換\n",
        "        sal = Image.fromarray(sal)\n",
        "        sal = sal.resize(img.size, resample=Image.LINEAR)\n",
        "\n",
        "        print()\n",
        "        #print(img_path) #あとで参照しやすいように画像のパスを表示\n",
        "\n",
        "        #plt.title('')\n",
        "        print('label: '+labels[test_dataset[j][1]])\n",
        "        print('pred:  '+'{}  {:.1f}%'.format(labels[c], 100*float(p)))\n",
        "        #plt.title('pred:'+'{}: { .1f}%'.format(labels[c], 100*float(p)))        \n",
        "        \n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        #グラフを1行2列に並べたうちの1番目\n",
        "        plt.subplots_adjust(wspace=0,hspace=0)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(np.array(sal), alpha=0.5, cmap='jet')\n",
        "\n",
        "        #元の画像を並べて表示\n",
        "        image = test_dataset[j][0]\n",
        "        image = image.permute(1, 2, 0)\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(image)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3QYH4diyu1h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Automated analysis**"
      ],
      "metadata": {
        "id": "MRzEYQQekA6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#保存用の空CSVを作成\n",
        "pt_num = []\n",
        "k=0\n",
        "for data in testset:\n",
        "    pt_num.append([k]*len(data))\n",
        "    k+=1\n",
        "\n",
        "img_num = []\n",
        "for data in testset:\n",
        "    img_num.append(list(range(len(data))))\n",
        "\n",
        "patient_num = list(itertools.chain.from_iterable(pt_num))\n",
        "img_num = list(itertools.chain.from_iterable(img_num))\n",
        "patient_path = list(itertools.chain.from_iterable(testset))\n",
        "patient_label = list(itertools.chain.from_iterable(testset_label))\n",
        "\n",
        "df_result = pd.DataFrame(index=[],columns=[])\n",
        "df_result = pd.DataFrame(index=[],columns=[\"pt_number\",\"img_number\", \"path\",\"label\", \"0\",\"1\",\"2\",\"3\",\"4\", \"prob_1\", \"prob_2\", \"prob_3\", \"prob_4\", \"prob_5\"])\n",
        "df_result[\"pt_number\"] = patient_num\n",
        "df_result[\"img_number\"] = img_num\n",
        "df_result[\"path\"] = patient_path\n",
        "df_result[\"label\"] = patient_label\n",
        "\n",
        "df_result.to_csv(r\"F:\\先天性緑内障\\result.csv\",encoding=\"shift_jis\", index=False)\n",
        "df_result"
      ],
      "metadata": {
        "id": "hJAkQ_Bun31w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.to_csv(result_csv_path,encoding=\"shift_jis\", index=False) #save as csv"
      ],
      "metadata": {
        "id": "yOJJ87DGGkOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Open reslut_csv\n",
        "with codecs.open(result_csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "        df_result = pd.read_csv(file, index_col=None, header=0)\n",
        "\n",
        "time_start = time.perf_counter()\n",
        "\n",
        "#pt,foldの初期値を入力（CSVに対応）\n",
        "pt=368\n",
        "fold=0\n",
        "\n",
        "for pt in range(pt,len(testset)): #指定したPtから開始\n",
        "    for fold in list(range(5)):\n",
        "        print(\"patient: \"+str(pt)+\", fold: \"+str(fold))\n",
        "\n",
        "        train_list = train_dataset_gla[pt][fold] + train_dataset_cont[pt][fold]\n",
        "        train_list_label = list(itertools.repeat(1, len(train_dataset_gla[pt][fold])))+list(itertools.repeat(0, len(train_dataset_cont[pt][fold])))\n",
        "        val_list = val_dataset_gla[pt][fold] + val_dataset_cont[pt][fold]\n",
        "        val_list_label = list(itertools.repeat(1, len(val_dataset_gla[pt][fold])))+list(itertools.repeat(0, len(val_dataset_cont[pt][fold])))\n",
        "        test_list = testset[pt]\n",
        "        test_list_label = testset_label[pt]\n",
        "\n",
        "        print(len(train_list))\n",
        "        print(len(val_list))\n",
        "        print(len(test_list))\n",
        "\n",
        "\n",
        "        #define dataset and dataloader\n",
        "        train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "        val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "        test_dataset = SimpleImageDataset(test_list, test_list_label, test_data_transforms)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "        val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "        test_loader = DataLoader(test_dataset, batch_size = 1, shuffle = False)\n",
        "\n",
        "        # show sample image\n",
        "        inputs, classes = next(iter(test_loader))\n",
        "        print(classes)\n",
        "        out = torchvision.utils.make_grid(inputs)\n",
        "        class_names = [\"cont\", \"gla\"]\n",
        "        imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "\n",
        "        model_ft = create_RepVGG_A2(deploy=False)\n",
        "        model_ft.load_state_dict(torch.load (\"F:\\先天性緑内障\\RepVGG-A2.pth\"))   \n",
        "        num_ftrs = model_ft.linear.in_features\n",
        "        model_ft.linear = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "        #GPU使用\n",
        "        model_ft = model_ft.to(device)\n",
        "\n",
        "        #損失関数を定義\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Observe that all parameters are being optimized\n",
        "        #https://blog.knjcode.com/adabound-memo/\n",
        "        #https://pypi.org/project/torch-optimizer/\n",
        "        from ranger_adabelief import RangerAdaBelief\n",
        "        optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "        model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=10, num_epochs=30)\n",
        "\n",
        "\n",
        "        # visualize the loss as the network trained\n",
        "        fig = plt.figure(figsize=(10,8))\n",
        "        plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
        "        plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
        "\n",
        "        # find position of lowest validation loss\n",
        "        minposs = valid_loss.index(min(valid_loss))+1 \n",
        "        plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('loss')\n",
        "        plt.ylim(0, 1.0) # consistent scale\n",
        "        plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        fig.savefig('loss_plot.png', bbox_inches='tight')\n",
        "\n",
        "        #Prediction for testset\n",
        "        model_ft.eval() # prep model for evaluation\n",
        "        targets, probs, preds =[], [], []\n",
        "        for image_tensor, target in test_loader:  \n",
        "              #target = target.squeeze(1)     \n",
        "              image_tensor = image_tensor.to(device)\n",
        "              target = target.to(device)\n",
        "              # forward pass: compute predicted outputs by passing inputs to the model\n",
        "              output = model_ft(image_tensor)\n",
        "              _, pred = torch.max(output, 1) \n",
        "            \n",
        "              prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "              prob = prob[0][1].cpu().detach() #probalility of being positive\n",
        "              print(prob)\n",
        "              print(pred) \n",
        "              \n",
        "              probs.append(prob)\n",
        "              preds.append(int(pred))  #予測結果\n",
        "              targets.append(int(target)) #ラベル\n",
        "        y_test = np.array(targets)\n",
        "        y_pred = np.array(preds)\n",
        "        y_prob = np.array(probs)\n",
        "        print(\"label\")\n",
        "        print(y_test)\n",
        "        print(\"pred\")\n",
        "        print(y_pred)\n",
        "        print(\"prob\")\n",
        "        print(y_prob)\n",
        "\n",
        "        #write result to df\n",
        "        row = 0\n",
        "        for i in testset[0:pt]:\n",
        "            row += len(i)\n",
        "        column = fold + 4\n",
        "        df_result.iloc[row:row+len(y_pred), column] = y_pred\n",
        "        column = fold + 9\n",
        "        df_result.iloc[row:row+len(y_pred), column] = y_prob\n",
        "        df_result.to_csv(result_csv_path,encoding=\"shift_jis\", index=False) #save as csv\n",
        "        \n",
        "\n",
        "        #経過時間を表示\n",
        "        time_end = time.perf_counter()\n",
        "        time_elapsed = (time_end - time_start)\n",
        "        print(\"Elapsed time: \"+str(time_elapsed))\n",
        "\n"
      ],
      "metadata": {
        "id": "wu9YkXhfJBFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**データセットをフォルダにコピー**"
      ],
      "metadata": {
        "id": "tWzsW2q6mnYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#データのパスを指定\n",
        "pt=0\n",
        "fold=0\n",
        "train_list_gla = train_dataset_gla[pt][fold] \n",
        "train_list_cont = train_dataset_cont[pt][fold]\n",
        "val_list_gla = val_dataset_gla[pt][fold]\n",
        "val_list_cont = val_dataset_cont[pt][fold]\n",
        "\n",
        "#パスを指定\n",
        "train_gla = r\"F:\\先天性緑内障\\データ引継ぎ\\stratified\\train\\gla\"\n",
        "train_cont = r\"F:\\先天性緑内障\\データ引継ぎ\\stratified\\train\\cont\"\n",
        "val_gla = r\"F:\\先天性緑内障\\データ引継ぎ\\stratified\\val\\gla\"\n",
        "val_cont = r\"F:\\先天性緑内障\\データ引継ぎ\\stratified\\val\\cont\"\n",
        "\n",
        "orig_path = [train_list_gla, train_list_cont, val_list_gla, val_list_cont]\n",
        "dst_path = [train_gla, train_cont, val_gla, val_cont]\n",
        "\n",
        "\n",
        "for folder in dst_path:\n",
        "    if os.path.exists(folder):\n",
        "        shutil.rmtree(folder)\n",
        "    os.makedirs(folder, exist_ok=True) \n",
        "\n",
        "for i in orig_path:\n",
        "    print(len(i))\n",
        "\n",
        "\n",
        "for orig_img_list, dst_folder in zip(orig_path, dst_path):\n",
        "    if not orig_img_list:\n",
        "        pass\n",
        "    else:\n",
        "        for origpath in orig_img_list:\n",
        "            basepath = os.path.basename(origpath)\n",
        "            dstpath = os.path.join(dst_folder, basepath)\n",
        "            shutil.copyfile(origpath, dstpath)\n",
        "            print(dstpath+ \" copied!\")"
      ],
      "metadata": {
        "id": "t-5NN-_smwNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**結果のCSVを開く**"
      ],
      "metadata": {
        "id": "anmRZaRx0acX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_result"
      ],
      "metadata": {
        "id": "HVZwL-S_xpla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with codecs.open(result_csv_path, \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
        "        df_result = pd.read_csv(file, index_col=None, header=0)\n",
        "df_result"
      ],
      "metadata": {
        "id": "xdO8ZJ-qhV-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "import sklearn; \n",
        "\n",
        "\n",
        "#sorted(sklearn.metrics.SCORERS.keys())\n",
        "\n",
        "\n",
        "#indexの内容を確認\n",
        "#print(df.columns.values.tolist())\n",
        "\n",
        "\n",
        "FEATURE_COLS=df_result.columns.values[1:].tolist()\n",
        "\n",
        "\"\"\"\n",
        "FEATURE_COLS=[\n",
        " 'cropped_A2',\n",
        " 'cropped_B3']\n",
        "\"\"\"\n",
        "\n",
        "print(FEATURE_COLS)\n",
        "\n",
        "# 訓練データとテストデータに分割する。\n",
        "from sklearn.model_selection import train_test_split\n",
        "# TODO:層別サンプリング train, test = train_test_split(df, test_size=0.20, stratify=df[\"町区分\"], random_state=100)\n",
        "train, test = train_test_split(df_result, test_size=0.20,random_state=100)\n",
        "\n",
        "X_train = train[FEATURE_COLS[8:13]]\n",
        "Y_train = train[\"label\"]\n",
        "X_test = test[FEATURE_COLS[8:13]]\n",
        "Y_test = test[\"label\"]\n"
      ],
      "metadata": {
        "id": "YWTX_kVS1Btc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7115129b-3393-4de9-db94-cca74f35f7fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\statsmodels\\compat\\pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
            "  from pandas import Int64Index as NumericIndex\n",
            "c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
            "  from pandas import MultiIndex, Int64Index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['img_number', 'path', 'label', '0', '1', '2', '3', '4', 'prob_1', 'prob_2', 'prob_3', 'prob_4', 'prob_5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "import sklearn; \n",
        "\n",
        "!pip install bayesian-optimization \n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "\n",
        "#　　精度確認\n",
        "# 自由度調整済みr2を算出\n",
        "def adjusted_r2(X,Y):\n",
        "    from sklearn.metrics import r2_score\n",
        "    import numpy as np\n",
        "    r_squared = r2_score(Y, X)\n",
        "    adjusted_r2 = 1 - (1-r_squared)*(len(Y)-1)/(len(Y)-2)\n",
        "    #yhat = model.predict(X) \\ #SS_Residual = sum((Y-yhat)**2) \\ #SS_Total = sum((Y-np.mean(Y))**2)\n",
        "    #r_squared = 1 - (float(SS_Residual))/ SS_Total\n",
        "    return adjusted_r2\n",
        "\n",
        "# 予測モデルの精度確認の各種指標を算出\n",
        "def get_model_evaluations(X_train,Y_train,X_test,Y_test):\n",
        "    from sklearn.metrics import explained_variance_score\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.metrics import mean_squared_log_error\n",
        "    from sklearn.metrics import median_absolute_error\n",
        "\n",
        "   # 評価指標確認\n",
        "   # 参考: https://funatsu-lab.github.io/open-course-ware/basic-theory/accuracy-index/\n",
        "    yhat_test = X_test\n",
        "\n",
        "    print(\"adjusted_r2(train)     :\" + str(adjusted_r2(X_train,Y_train)))\n",
        "    print(\"adjusted_r2(test)      :\" + str(adjusted_r2(X_test,Y_test)))   \n",
        "    #print(\"平均誤差率(test)       :\" + str(np.mean(abs(Y_test / yhat_test - 1)))) \n",
        "    print(\"MAE(test)              :\" + str(mean_absolute_error(Y_test, yhat_test)))\n",
        "    print(\"MedianAE(test)         :\" + str(median_absolute_error(Y_test, yhat_test)))\n",
        "    print(\"RMSE(test)             :\" + str(np.sqrt(mean_squared_error(Y_test, yhat_test))))\n",
        "    print(\"RMSE(test) / MAE(test) :\" + str(np.sqrt(mean_squared_error(Y_test, yhat_test)) / mean_absolute_error(Y_test, yhat_test))) #better if result = 1.253\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in FEATURE_COLS[8:13]:\n",
        "    X_train = train[i]\n",
        "    Y_train = train[\"label\"]\n",
        "    X_test = test[i]\n",
        "    Y_test = test[\"label\"]\n",
        "    print(str(i))\n",
        "    get_model_evaluations(X_train,Y_train,X_test,Y_test)\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#後の解析のためにそれぞれの項目を戻しておく\n",
        "X_train = train[FEATURE_COLS[8:13]]\n",
        "Y_train = train[\"label\"]\n",
        "X_test = test[FEATURE_COLS[8:13]]\n",
        "Y_test = test[\"label\"]\n"
      ],
      "metadata": {
        "id": "CYwTK7oJ2tIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "\n",
        "\"\"\"XGBoost で二値分類するサンプルコード\"\"\"\n",
        "\n",
        "X_train = train[FEATURE_COLS[8:13]]\n",
        "Y_train = train[\"label\"]\n",
        "X_test = test[FEATURE_COLS[8:13]]\n",
        "Y_test = test[\"label\"]\n",
        "\n",
        "\n",
        "# XGBoost が扱うデータセットの形式に直す\n",
        "dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=Y_test)\n",
        "# 学習用のパラメータ\n",
        "xgb_params = {\n",
        "    # 二値分類問題\n",
        "    'objective': 'binary:logistic',\n",
        "    # 評価指標\n",
        "    'eval_metric': 'logloss',\n",
        "}\n",
        "# モデルを学習する\n",
        "bst = xgb.train(xgb_params,\n",
        "                dtrain,\n",
        "                num_boost_round=100,  # 学習ラウンド数は適当\n",
        "                )\n",
        "# 検証用データが各クラスに分類される確率を計算する\n",
        "Y_pred_proba = bst.predict(dtest)\n",
        "# しきい値 0.5 で 0, 1 に丸める\n",
        "Y_pred = np.where(Y_pred_proba > 0.5, 1, 0)\n",
        "# 精度 (Accuracy) を検証する\n",
        "acc = accuracy_score(Y_test, Y_pred)\n",
        "print('Accuracy:', acc)\n",
        "\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
        "print(tp, fn, fp, tn)\n",
        "\n",
        "def specificity_score(label, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "\n",
        "print('confusion matrix = \\n', confusion_matrix(Y_test, Y_pred))\n",
        "print(f'Accuracy : {accuracy_score(Y_test, Y_pred)}')\n",
        "print(f'Precision (true positive rate) : {precision_score(Y_test, Y_pred)}')\n",
        "print(f'Recall (sensitivity): {recall_score(Y_test, Y_pred)}')\n",
        "print(f'Specificity : {specificity_score(Y_test, Y_pred)}')\n",
        "print(f'F1 score : {f1_score(Y_test, Y_pred)}')\n",
        "\n",
        "\n",
        "#ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred_proba)     \n",
        "plt.plot(fpr, tpr, marker='o')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.grid()\n",
        "print(f'Area_under_ROC : {roc_auc_score(Y_test, Y_pred_proba)}')\n",
        "#plt.savefig('plots/roc_curve.png')\n"
      ],
      "metadata": {
        "id": "pto6ZDKycgFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "specificity = []\n",
        "f1score = []\n",
        "area_u_ROC = []\n",
        "\n",
        "\n",
        "for i in range(8,13):\n",
        "    print(\"fold\",i)\n",
        "    X = df_result[FEATURE_COLS[i]]\n",
        "    Y = df_result[\"label\"]\n",
        "\n",
        "    Y_pred_proba = X\n",
        "    Y_pred = np.where(Y_pred_proba >= 0.5, 1, 0)\n",
        "\n",
        "    acc = accuracy_score(Y, Y_pred)\n",
        "    print('Accuracy:',acc)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(Y, Y_pred).ravel()\n",
        "    print(tp, fn, fp, tn)\n",
        "\n",
        "    def specificity_score(label, pred):\n",
        "        tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "        return tn / (tn + fp)\n",
        "\n",
        "\n",
        "    print('confusion matrix = \\n', confusion_matrix(Y, Y_pred))\n",
        "    print(f'Accuracy : {accuracy_score(Y, Y_pred)}')\n",
        "    print(f'Precision (true positive rate) : {precision_score(Y, Y_pred)}')\n",
        "    print(f'Recall (sensitivity): {recall_score(Y, Y_pred)}')\n",
        "    print(f'Specificity : {specificity_score(Y, Y_pred)}')\n",
        "    print(f'F1 score : {f1_score(Y, Y_pred)}')\n",
        "\n",
        "    #ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(Y, Y_pred_proba)     \n",
        "    plt.plot(fpr, tpr, marker='o')\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.grid()\n",
        "    print(f'Area_under_ROC : {roc_auc_score(Y, Y_pred_proba)}')\n",
        "    #plt.savefig('plots/roc_curve.png')\n",
        "\n",
        "    accuracy.append(accuracy_score(Y, Y_pred))\n",
        "    precision.append(precision_score(Y, Y_pred))\n",
        "    recall.append(recall_score(Y, Y_pred))\n",
        "    specificity.append(specificity_score(Y, Y_pred))\n",
        "    f1score.append(f1_score(Y, Y_pred))\n",
        "    area_u_ROC.append(roc_auc_score(Y, Y_pred_proba))\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "print(\"Result of 5-fold crossvalidation\")\n",
        "print(\"accuracy: \", statistics.mean(accuracy))\n",
        "print(\"precision: \", statistics.mean(precision))\n",
        "print(\"recall (sensitivity): \", statistics.mean(recall))\n",
        "print(\"specifiity: \", statistics.mean(specificity))\n",
        "print(\"f1_score: \", statistics.mean(f1score))\n",
        "print(\"area_u_ROC: \", statistics.mean(area_u_ROC))"
      ],
      "metadata": {
        "id": "JxVZZLTNv_Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC curve描き直し\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 1:\n",
        "                  y_true.append(1)\n",
        "            elif i == 0:\n",
        "                  y_true.append(0)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "for i in range(8,13):\n",
        "    print(\"fold\",i)\n",
        "    X = df_result[FEATURE_COLS[i]]\n",
        "    Y = df_result[\"label\"]\n",
        "\n",
        "    Y_pred_proba = X\n",
        "    Y_pred = np.where(Y_pred_proba >= 0.5, 1, 0)\n",
        "\n",
        "    label_list_list.append(Y)\n",
        "    model_pred_prob_list.append(Y_pred_proba)\n",
        "\n",
        "#Draw ROC curve\n",
        "roc_label_list = list(range(5))\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n"
      ],
      "metadata": {
        "id": "ZBDF-nm0oJll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bayesian Optimization\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# BaysianOptimizationで最適化する関数を定義する\n",
        "def get_model_predictions(X_test, model):\n",
        "    yhat_test = model.predict(X_test)\n",
        "    return yhat_test\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# BaysianOptimizationで最適化する関数を定義する\n",
        "def xgb_regressor(max_depth, min_child_weight, gamma, subsample, colsample_bytree,reg_alpha, n_estimators, reg_lambda,learning_rate):\n",
        "\n",
        "    params = {'max_depth':int(max_depth),\n",
        "                'min_child_weight':int(min_child_weight),\n",
        "                'gamma':gamma,\n",
        "                'subsample':subsample,\n",
        "                'colsample_bytree':colsample_bytree,\n",
        "                'reg_alpha':reg_alpha,\n",
        "                'n_estimators':int(n_estimators),\n",
        "                'reg_lambda':reg_lambda,\n",
        "                'learning_rate':learning_rate\n",
        "                }\n",
        "    model = xgb.XGBRegressor(**params,\n",
        "                            early_stopping_rounds=50,\n",
        "                            eval_set=[(X_test, Y_test)],\n",
        "                            eval_metric='rmse',\n",
        "                            silent=False,\n",
        "                            n_jobs=-1\n",
        "                            )\n",
        "\n",
        "    Y_pred_cv = cross_val_predict(model,X_train,Y_train,cv=5, n_jobs=-1)\n",
        "    rmse_cv = np.sqrt(mean_squared_error(Y_train, Y_pred_cv))\n",
        "\n",
        "    return -rmse_cv\n",
        "\n",
        "#ベイズ最適化で探索するパラメータ空間を定義する\n",
        "xgb_bo = BayesianOptimization(xgb_regressor,\n",
        "                            {'max_depth':(3,8),\n",
        "                            'min_child_weight':(1,5),\n",
        "                            'gamma':(0,0.5),\n",
        "                            'subsample':(0.6,1),\n",
        "                            'colsample_bytree':(0.6,1),\n",
        "                            'reg_alpha':(1e-5,100),\n",
        "                            'n_estimators':(1000,2000),\n",
        "                            'reg_lambda':(1e-5,1),\n",
        "                            'learning_rate':(0.1,0.3)\n",
        "                            })\n",
        "\n",
        "#ベイズ最適化を実行（scoreが最大となるようにパラメータを探索していく）\n",
        "#init_point：初期に探索する点数\n",
        "#acq:獲得関数。EIは(expected improvement)\n",
        "xgb_bo.maximize(init_points=5, n_iter=200, acq='ei')\n",
        "\n",
        "#最もスコアのよかったパラメータの値を取得する。\n",
        "optimized_params = xgb_bo.max['params']\n",
        "\n",
        "#整数のパラメータは変換\n",
        "optimized_params['max_depth'] = int(optimized_params['max_depth'])\n",
        "optimized_params['min_child_weight'] = int(optimized_params['min_child_weight'])\n",
        "optimized_params['n_estimators'] = int(optimized_params['n_estimators'])\n",
        "\n",
        "#調整したパラメータで精度検証する\n",
        "opt_model = xgb.XGBRegressor()\n",
        "opt_model.set_params(**optimized_params)\n",
        "opt_model.fit(X_train, Y_train)\n",
        "#y_pred_train = opt_model.predict(X_train)\n",
        "#y_pred_test = opt_model.predict(X_test)\n",
        "bestmodel = opt_model\n",
        "print(bestmodel)\n",
        "\n",
        "\"\"\"\n",
        "# 学習モデルの評価（RMSEを計算）\n",
        "print('RMSE(train data):',round(np.sqrt(mean_squared_error(Y_train, Y_pred_train)),3))\n",
        "print('RMSE(test data):',round(np.sqrt(mean_squared_error(Y_test, Y_pred_test)),3))\n",
        "#output\n",
        "#RMSE(train data): 0.426\n",
        "#RMSE(test data): 2.266\n",
        "#CPU times: user 32.1 s, sys: 4.64 s, total: 36.7 s\n",
        "#Wall time: 11min 42s\n",
        "\n",
        "\"\"\"\n",
        "#ここから学習モデルの評価\n",
        "\n",
        "# 検証用データが各クラスに分類される確率を計算する\n",
        "Y_pred_proba = bestmodel.predict(X_test)\n",
        "# しきい値 0.5 で 0, 1 に丸める\n",
        "Y_pred = np.where(Y_pred_proba > 0.5, 1, 0)\n",
        "# 精度 (Accuracy) を検証する\n",
        "acc = accuracy_score(Y_test, Y_pred)\n",
        "print('Accuracy:', acc)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
        "print(tp, fn, fp, tn)\n",
        "\n",
        "def specificity_score(label, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "print('confusion matrix = \\n', confusion_matrix(Y_test, Y_pred))\n",
        "print(f'Accuracy : {accuracy_score(Y_test, Y_pred)}')\n",
        "print(f'Precision (true positive rate) : {precision_score(Y_test, Y_pred)}')\n",
        "print(f'Recall (sensitivity): {recall_score(Y_test, Y_pred)}')\n",
        "print(f'Specificity : {specificity_score(Y_test, Y_pred)}')\n",
        "print(f'F1 score : {f1_score(Y_test, Y_pred)}')\n",
        "\n",
        "\n",
        "#ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred_proba)     \n",
        "plt.plot(fpr, tpr, marker='o')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.grid()\n",
        "print(f'Area_under_ROC : {roc_auc_score(Y_test, Y_pred_proba)}')\n",
        "#plt.savefig('plots/roc_curve.png')"
      ],
      "metadata": {
        "id": "y-sxn3J5t7_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**患者ごとの正答率**"
      ],
      "metadata": {
        "id": "Cllzo5UeazmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "_XkW7jUva_GC",
        "outputId": "bb3b089d-2420-44da-e074-4d59685c7be8"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     pt_number  img_number                                               path  \\\n",
              "0            0           0  F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\\...   \n",
              "1            0           1  F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\\...   \n",
              "2            0           2  F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\\...   \n",
              "3            0           3  F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\\...   \n",
              "4            0           4  F:\\先天性緑内障\\dataset_for_article_250px\\gla_exo\\15...   \n",
              "..         ...         ...                                                ...   \n",
              "811        636           0  F:\\先天性緑内障\\dataset_for_article_250px\\cont_ortho...   \n",
              "812        637           0  F:\\先天性緑内障\\dataset_for_article_250px\\cont_ortho...   \n",
              "813        638           0  F:\\先天性緑内障\\dataset_for_article_250px\\cont_eso\\9...   \n",
              "814        639           0  F:\\先天性緑内障\\dataset_for_article_250px\\cont_exo\\9...   \n",
              "815        640           0  F:\\先天性緑内障\\dataset_for_article_250px\\cont_exo\\9...   \n",
              "\n",
              "     label    0    1    2    3    4    prob_1    prob_2    prob_3    prob_4  \\\n",
              "0        1  0.0  0.0  0.0  0.0  0.0  0.001319  0.000017  0.001339  0.000282   \n",
              "1        1  0.0  0.0  0.0  0.0  0.0  0.001970  0.003541  0.001281  0.000181   \n",
              "2        1  0.0  0.0  0.0  0.0  0.0  0.061344  0.055671  0.013234  0.000334   \n",
              "3        1  0.0  0.0  0.0  0.0  0.0  0.086217  0.001450  0.000020  0.000725   \n",
              "4        1  0.0  0.0  0.0  0.0  0.0  0.041507  0.006979  0.034302  0.003810   \n",
              "..     ...  ...  ...  ...  ...  ...       ...       ...       ...       ...   \n",
              "811      0  0.0  0.0  0.0  0.0  0.0  0.036239  0.000245  0.008298  0.162532   \n",
              "812      0  0.0  0.0  0.0  0.0  0.0  0.005409  0.006200  0.000063  0.021914   \n",
              "813      0  0.0  0.0  0.0  0.0  0.0  0.025551  0.040427  0.000252  0.000442   \n",
              "814      0  0.0  0.0  0.0  0.0  0.0  0.001012  0.000056  0.047037  0.000660   \n",
              "815      0  0.0  0.0  0.0  0.0  0.0  0.029085  0.004831  0.000009  0.033936   \n",
              "\n",
              "       prob_5  \n",
              "0    0.000065  \n",
              "1    0.059821  \n",
              "2    0.057779  \n",
              "3    0.000123  \n",
              "4    0.002042  \n",
              "..        ...  \n",
              "811  0.001722  \n",
              "812  0.000362  \n",
              "813  0.005331  \n",
              "814  0.000009  \n",
              "815  0.000115  \n",
              "\n",
              "[816 rows x 14 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pt_number</th>\n",
              "      <th>img_number</th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>prob_1</th>\n",
              "      <th>prob_2</th>\n",
              "      <th>prob_3</th>\n",
              "      <th>prob_4</th>\n",
              "      <th>prob_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\\...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.001339</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.000065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\\...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001970</td>\n",
              "      <td>0.003541</td>\n",
              "      <td>0.001281</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.059821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\\...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.061344</td>\n",
              "      <td>0.055671</td>\n",
              "      <td>0.013234</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.057779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\\...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.086217</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.000123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\gla_exo\\15...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.041507</td>\n",
              "      <td>0.006979</td>\n",
              "      <td>0.034302</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.002042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>636</td>\n",
              "      <td>0</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\cont_ortho...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036239</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.008298</td>\n",
              "      <td>0.162532</td>\n",
              "      <td>0.001722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>812</th>\n",
              "      <td>637</td>\n",
              "      <td>0</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\cont_ortho...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005409</td>\n",
              "      <td>0.006200</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.021914</td>\n",
              "      <td>0.000362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>813</th>\n",
              "      <td>638</td>\n",
              "      <td>0</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\cont_eso\\9...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025551</td>\n",
              "      <td>0.040427</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.005331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>639</td>\n",
              "      <td>0</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\cont_exo\\9...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.047037</td>\n",
              "      <td>0.000660</td>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>815</th>\n",
              "      <td>640</td>\n",
              "      <td>0</td>\n",
              "      <td>F:\\先天性緑内障\\dataset_for_article_250px\\cont_exo\\9...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.029085</td>\n",
              "      <td>0.004831</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.033936</td>\n",
              "      <td>0.000115</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>816 rows × 14 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "threshold = 0.5 #判定基準。ここは先に入力しておく\n",
        "#################################################\n",
        "\n",
        "df_pt_analysis = pd.DataFrame(index=[],columns=[])\n",
        "df_pt_analysis = pd.DataFrame(index=[],columns=[\"pt_number\",\"label\",\"threshold\",\"number_of_img\", \"correct\", \"judgement\", \"result\"])\n",
        "\n",
        "#gla群\n",
        "pt_gla = df_result.loc[df_result[\"label\"]==1][\"pt_number\"].drop_duplicates().tolist()\n",
        "\n",
        "#cont群（数をgla群に揃えるよう、ランダムにピックアップ）\n",
        "pt_cont= df_result.loc[df_result[\"label\"]==0][\"pt_number\"].drop_duplicates().tolist()\n",
        "pt_cont = sorted(random.sample(pt_cont, len(pt_gla)))\n",
        "\n",
        "\n",
        "for i in pt_gla:\n",
        "    pt_number = i\n",
        "    label = 1\n",
        "    threshold = threshold\n",
        "    number_of_img = df_result[\"pt_number\"][df_result[\"pt_number\"] == i].count()\n",
        "    \n",
        "\n",
        "\"\"\"\n",
        "df_pt_analysis[\"pt_number\"] = [0, 1, 2]\n",
        "df_pt_analysis[\"label\"] = [0, 1, 3]\n",
        "df_pt_analysis\n",
        "\"\"\"\n",
        "print(pt_cont)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwAhKEE8bAUj",
        "outputId": "267443fa-3085-46ab-8eba-d75a4cd67890"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 48, 50, 72, 83, 117, 133, 146, 185, 190, 220, 233, 241, 246, 257, 279, 297, 298, 410, 436, 452, 480, 491, 514, 572, 595, 606, 615, 623, 631]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp = df_result.loc[df_result[\"pt_number\"]==0] #患者の画像リストをすべて抜き出す\n",
        "df_prob = df_temp[[\"prob_1\",\"prob_2\", \"prob_3\", \"prob_4\", \"prob_5\"]].values.flatten().tolist() #probalilityの項目をnumpyに変換して1次元に変換し、リストに戻す\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNPcPr9PnKE1",
        "outputId": "d7368880-8753-42c0-e176-6228308d05ff"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0013190309982746,\n",
              " 1.6998274077195674e-05,\n",
              " 0.0013392319669947,\n",
              " 0.0002823222021106,\n",
              " 6.482706521637738e-05,\n",
              " 0.0019698040559887,\n",
              " 0.0035410255659371,\n",
              " 0.0012805875157937,\n",
              " 0.0001811943948268,\n",
              " 0.0598208643496036,\n",
              " 0.0613441243767738,\n",
              " 0.0556709989905357,\n",
              " 0.013233533129096,\n",
              " 0.000334009702783,\n",
              " 0.0577794723212718,\n",
              " 0.0862165912985801,\n",
              " 0.0014498290838673,\n",
              " 1.9504370357026343e-05,\n",
              " 0.0007254891097545,\n",
              " 0.0001228586042998,\n",
              " 0.0415065251290798,\n",
              " 0.0069791553542017,\n",
              " 0.0343022122979164,\n",
              " 0.0038098872173577,\n",
              " 0.0020418183412402,\n",
              " 0.0073239989578723,\n",
              " 0.0003517969162203,\n",
              " 0.0003970740071963,\n",
              " 0.0009738443768583,\n",
              " 0.0002009543241001,\n",
              " 0.0061536133289337,\n",
              " 1.4985610505391378e-05,\n",
              " 2.179050352424383e-05,\n",
              " 0.0003131002304144,\n",
              " 3.888247192662675e-06]"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Emx_Nw5NnJ62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}