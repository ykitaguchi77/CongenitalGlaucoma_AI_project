{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZIsvFySgdko5RCew6OEHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CongenitalGlaucoma_AI_project/blob/main/Cornea_train_annotated_ellipse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train model to predict annoted ellipse**"
      ],
      "metadata": {
        "id": "m4mVve6eaEFh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rag9hPu4aDeF",
        "outputId": "6cd47daa-1170-4865-e33a-4fdbda119f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import re\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import pickle\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torch_optimizer --q\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "random_seed = 2 #shuffleのシード\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "#GDriveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xml_path = \"/content/drive/MyDrive/Deep_learning/Congenital_Glaucoma/glaucoma_cornea_ellipse/annotations.xml\""
      ],
      "metadata": {
        "id": "K-DwGOQHcGLA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "# XMLデータをパースする\n",
        "tree = ET.parse(xml_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "# イメージの情報を含む行のリストを作成する\n",
        "image_data = []\n",
        "for image in root.findall('image'):\n",
        "    try:\n",
        "        attributes = image.attrib\n",
        "        image_id = attributes.get('id', None)\n",
        "        name = attributes.get('name', None)\n",
        "        width = attributes.get('width', None)\n",
        "        height = attributes.get('height', None)\n",
        "        ellipse = image.find('ellipse')\n",
        "        label = ellipse.get('label', None)\n",
        "        occluded = ellipse.get('occluded', None)\n",
        "        source = ellipse.get('source', None)\n",
        "        cx = ellipse.get('cx', None)\n",
        "        cy = ellipse.get('cy', None)\n",
        "        rx = ellipse.get('rx', None)\n",
        "        ry = ellipse.get('ry', None)\n",
        "        rotation = ellipse.get('rotation', None)\n",
        "        z_order = ellipse.get('z_order', None)\n",
        "        image_data.append([image_id, name, width, height, label, occluded, source, cx, cy, rx, ry, rotation, z_order])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# データフレームに変換する\n",
        "df = pd.DataFrame(image_data, columns=['image_id', 'name', 'width', 'height', 'label', 'occluded', 'source', 'cx', 'cy', 'rx', 'ry', 'rotation', 'z_order'])\n",
        "\n",
        "# \"nonetype\"オブジェクトを削除、角度を±にする\n",
        "angle_list = []\n",
        "for i, angle in enumerate(df[\"rotation\"].apply(pd.to_numeric, errors='coerce')):\n",
        "    if math.isnan(angle):\n",
        "        angle = 0\n",
        "    elif angle <= 180:\n",
        "        angle = angle\n",
        "    else:\n",
        "        angle = angle - 360\n",
        "    angle_list.append(angle)\n",
        "df[\"rotation\"] = angle_list\n",
        "\n",
        "\n",
        "# 患者番号を追加\n",
        "df[\"patient\"] = [i.split(\"-\")[0] for i in df[\"name\"]]\n",
        "\n"
      ],
      "metadata": {
        "id": "3261a7cxfgzt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "rAcqoffA-Aor",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "228a46fc-e93d-411f-bcd0-0ada1840a1ae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    image_id        name width height           label occluded  source  \\\n",
              "0          0     0-1.jpg   640    640  corneal margin        0  manual   \n",
              "1          1     1-0.jpg   640    640  corneal margin        0  manual   \n",
              "2          2    10-1.jpg   640    640  corneal margin        0  manual   \n",
              "3          3   100-0.jpg   640    640  corneal margin        0  manual   \n",
              "4          4  1000-0.jpg   640    640  corneal margin        0  manual   \n",
              "..       ...         ...   ...    ...             ...      ...     ...   \n",
              "544      546     7-1.jpg   640    640  corneal margin        0  manual   \n",
              "545      547    70-0.jpg   640    640  corneal margin        0  manual   \n",
              "546      548   700-0.jpg   640    640  corneal margin        0  manual   \n",
              "547      549   701-1.jpg   640    640  corneal margin        0  manual   \n",
              "548      550   702-0.jpg   640    640  corneal margin        0  manual   \n",
              "\n",
              "         cx      cy      rx      ry  rotation z_order patient  \n",
              "0    312.18  312.65  109.45  108.95       0.0       0       0  \n",
              "1    387.20  317.79  124.21  128.59       0.0       0       1  \n",
              "2    298.68  268.19  163.88  163.17      -7.4       0      10  \n",
              "3    410.59  309.76  135.89  135.86       0.0       0     100  \n",
              "4    392.33  320.67  138.87  137.39       0.0       0    1000  \n",
              "..      ...     ...     ...     ...       ...     ...     ...  \n",
              "544  400.30  306.80  157.35  165.35       0.0       0       7  \n",
              "545  430.65  314.42  154.00  148.23      24.6       0      70  \n",
              "546  384.63  299.37  135.54  131.02      18.1       0     700  \n",
              "547  316.99  299.22  138.45  131.61     -16.6       0     701  \n",
              "548  369.37  301.45  119.18  119.85       0.0       0     702  \n",
              "\n",
              "[549 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2fc715c2-59c0-4309-b10b-9e8b9144e7f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>name</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>label</th>\n",
              "      <th>occluded</th>\n",
              "      <th>source</th>\n",
              "      <th>cx</th>\n",
              "      <th>cy</th>\n",
              "      <th>rx</th>\n",
              "      <th>ry</th>\n",
              "      <th>rotation</th>\n",
              "      <th>z_order</th>\n",
              "      <th>patient</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0-1.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>312.18</td>\n",
              "      <td>312.65</td>\n",
              "      <td>109.45</td>\n",
              "      <td>108.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1-0.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>387.20</td>\n",
              "      <td>317.79</td>\n",
              "      <td>124.21</td>\n",
              "      <td>128.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10-1.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>298.68</td>\n",
              "      <td>268.19</td>\n",
              "      <td>163.88</td>\n",
              "      <td>163.17</td>\n",
              "      <td>-7.4</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>100-0.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>410.59</td>\n",
              "      <td>309.76</td>\n",
              "      <td>135.89</td>\n",
              "      <td>135.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1000-0.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>392.33</td>\n",
              "      <td>320.67</td>\n",
              "      <td>138.87</td>\n",
              "      <td>137.39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>544</th>\n",
              "      <td>546</td>\n",
              "      <td>7-1.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>400.30</td>\n",
              "      <td>306.80</td>\n",
              "      <td>157.35</td>\n",
              "      <td>165.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545</th>\n",
              "      <td>547</td>\n",
              "      <td>70-0.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>430.65</td>\n",
              "      <td>314.42</td>\n",
              "      <td>154.00</td>\n",
              "      <td>148.23</td>\n",
              "      <td>24.6</td>\n",
              "      <td>0</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>548</td>\n",
              "      <td>700-0.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>384.63</td>\n",
              "      <td>299.37</td>\n",
              "      <td>135.54</td>\n",
              "      <td>131.02</td>\n",
              "      <td>18.1</td>\n",
              "      <td>0</td>\n",
              "      <td>700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547</th>\n",
              "      <td>549</td>\n",
              "      <td>701-1.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>316.99</td>\n",
              "      <td>299.22</td>\n",
              "      <td>138.45</td>\n",
              "      <td>131.61</td>\n",
              "      <td>-16.6</td>\n",
              "      <td>0</td>\n",
              "      <td>701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548</th>\n",
              "      <td>550</td>\n",
              "      <td>702-0.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>640</td>\n",
              "      <td>corneal margin</td>\n",
              "      <td>0</td>\n",
              "      <td>manual</td>\n",
              "      <td>369.37</td>\n",
              "      <td>301.45</td>\n",
              "      <td>119.18</td>\n",
              "      <td>119.85</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>702</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>549 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fc715c2-59c0-4309-b10b-9e8b9144e7f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2fc715c2-59c0-4309-b10b-9e8b9144e7f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2fc715c2-59c0-4309-b10b-9e8b9144e7f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, df, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.df = df\n",
        "        self.item_dict = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = df[df[\"name\"]==os.path.basename(image_path)].loc[:, \"cx\":\"rotation\"].apply(pd.to_numeric, errors='coerce')\n",
        "        target = target.values.tolist()[0]\n",
        "        target = torch.tensor(target)\n",
        "        return tensor_image, target\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "        \n",
        "        #running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            \n",
        "            #普通はこちらを使う\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            \n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            #running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "        #print()   \n",
        "        #train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        #running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "           \n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            #running_corrects += torch.sum(preds==labels)\n",
        "        #val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(num_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     #f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' \n",
        "                     #f'valid_acc: {val_acc:.5f}'\n",
        "                     )\n",
        "        print(print_msg)\n",
        "\n",
        "        \"\"\"\n",
        "        #Scheduler step for ReduceLROnPlateau\n",
        "        scheduler.step(valid_loss)\n",
        "        \"\"\"\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n"
      ],
      "metadata": {
        "id": "lRhvvaN-f0u1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 15\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "\n",
        "# define image augmentation\n",
        "train_data_transforms = transforms.Compose([\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "image_dir = \"/content/drive/MyDrive/Deep_learning/Congenital_Glaucoma/glaucoma_cornea_ellipse/images\"\n",
        "train_X = [f\"{image_dir}/{basename}\" for basename in df[\"name\"]]\n",
        "groups = df[\"patient\"]\n",
        "\n",
        "gkf = GroupKFold(n_splits = 5)\n",
        "\n",
        "for train_idx, val_idx in gkf.split(train_X, groups=groups):\n",
        "    train_list = [train_X[i] for i in train_idx]\n",
        "    val_list = [train_X[i] for i in val_idx]\n",
        "\n",
        "\n",
        "#define dataset and dataloader\n",
        "train_dataset = SimpleImageDataset(train_list, df, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, df, val_data_transforms)\n",
        "test_dataset = SimpleImageDataset(val_list, df, test_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "test_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False, pin_memory=True, num_workers=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "asMVJbqEoK9y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model(\"mobilenetv3_large_100\", pretrained = True, num_classes = 5)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "!pip install ranger_adabelief --q\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1rSNBSslykk",
        "outputId": "14e318ab-63af-47e7-ed15-d50d5d916fc5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_large_100_ra-f55367f5.pth\" to /root/.cache/torch/hub/checkpoints/mobilenetv3_large_100_ra-f55367f5.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=30, num_epochs=300)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rm_6sfI1K0K",
        "outputId": "8ff8b6ab-ee25-407b-df8b-42ee5a64fac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Epoch: [  0/300\n",
            "train_loss: 28383.30808 valid_loss: 16564.12598 \n",
            "Validation loss decreased (inf --> 16564.125977).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [  1/300\n",
            "train_loss: 6835.16034 valid_loss: 2251.22363 \n",
            "Validation loss decreased (16564.125977 --> 2251.223633).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [  2/300\n",
            "train_loss: 889.02954 valid_loss: 1011.57086 \n",
            "Validation loss decreased (2251.223633 --> 1011.570862).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [  3/300\n",
            "train_loss: 741.19775 valid_loss: 944.89284 \n",
            "Validation loss decreased (1011.570862 --> 944.892840).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [  4/300\n",
            "train_loss: 654.85631 valid_loss: 946.95620 \n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [  5/300\n",
            "train_loss: 573.99032 valid_loss: 850.37946 \n",
            "Validation loss decreased (944.892840 --> 850.379460).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [  6/300\n",
            "train_loss: 539.81388 valid_loss: 852.66007 \n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [  7/300\n",
            "train_loss: 490.04006 valid_loss: 813.65502 \n",
            "Validation loss decreased (850.379460 --> 813.655016).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [  8/300\n",
            "train_loss: 427.05856 valid_loss: 754.33788 \n",
            "Validation loss decreased (813.655016 --> 754.337882).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [  9/300\n",
            "train_loss: 398.60340 valid_loss: 766.67449 \n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 10/300\n",
            "train_loss: 351.87453 valid_loss: 638.12814 \n",
            "Validation loss decreased (754.337882 --> 638.128139).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 11/300\n",
            "train_loss: 311.49907 valid_loss: 623.09818 \n",
            "Validation loss decreased (638.128139 --> 623.098184).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 12/300\n",
            "train_loss: 299.64055 valid_loss: 635.60005 \n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 13/300\n",
            "train_loss: 280.36240 valid_loss: 704.44178 \n",
            "EarlyStopping counter: 2 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 14/300\n",
            "train_loss: 282.34528 valid_loss: 577.99978 \n",
            "Validation loss decreased (623.098184 --> 577.999775).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 15/300\n",
            "train_loss: 229.21935 valid_loss: 668.03931 \n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 16/300\n",
            "train_loss: 242.84454 valid_loss: 469.91670 \n",
            "Validation loss decreased (577.999775 --> 469.916704).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 17/300\n",
            "train_loss: 236.23321 valid_loss: 443.36867 \n",
            "Validation loss decreased (469.916704 --> 443.368668).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 18/300\n",
            "train_loss: 199.28761 valid_loss: 597.63366 \n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 19/300\n",
            "train_loss: 179.79819 valid_loss: 557.45724 \n",
            "EarlyStopping counter: 2 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 20/300\n",
            "train_loss: 182.35219 valid_loss: 504.07525 \n",
            "EarlyStopping counter: 3 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 21/300\n",
            "train_loss: 165.74759 valid_loss: 522.51814 \n",
            "EarlyStopping counter: 4 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 22/300\n",
            "train_loss: 164.81728 valid_loss: 517.97855 \n",
            "EarlyStopping counter: 5 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 23/300\n",
            "train_loss: 158.37866 valid_loss: 493.14853 \n",
            "EarlyStopping counter: 6 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 24/300\n",
            "train_loss: 167.62648 valid_loss: 514.45307 \n",
            "EarlyStopping counter: 7 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 25/300\n",
            "train_loss: 142.77471 valid_loss: 528.85477 \n",
            "EarlyStopping counter: 8 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 26/300\n",
            "train_loss: 124.31909 valid_loss: 556.63103 \n",
            "EarlyStopping counter: 9 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 27/300\n",
            "train_loss: 153.63851 valid_loss: 565.99886 \n",
            "EarlyStopping counter: 10 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 28/300\n",
            "train_loss: 152.27075 valid_loss: 582.57607 \n",
            "EarlyStopping counter: 11 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 29/300\n",
            "train_loss: 119.93439 valid_loss: 516.72286 \n",
            "EarlyStopping counter: 12 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 30/300\n",
            "train_loss: 125.68825 valid_loss: 485.15683 \n",
            "EarlyStopping counter: 13 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 31/300\n",
            "train_loss: 120.21700 valid_loss: 524.03263 \n",
            "EarlyStopping counter: 14 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 32/300\n",
            "train_loss: 135.32255 valid_loss: 502.47230 \n",
            "EarlyStopping counter: 15 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 33/300\n",
            "train_loss: 125.79359 valid_loss: 478.75181 \n",
            "EarlyStopping counter: 16 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 34/300\n",
            "train_loss: 129.08363 valid_loss: 510.00867 \n",
            "EarlyStopping counter: 17 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 35/300\n",
            "train_loss: 112.19231 valid_loss: 495.83506 \n",
            "EarlyStopping counter: 18 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 36/300\n",
            "train_loss: 114.96113 valid_loss: 480.39127 \n",
            "EarlyStopping counter: 19 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 37/300\n",
            "train_loss: 127.96269 valid_loss: 462.45760 \n",
            "EarlyStopping counter: 20 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 38/300\n",
            "train_loss: 119.42605 valid_loss: 420.64905 \n",
            "Validation loss decreased (443.368668 --> 420.649050).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 39/300\n",
            "train_loss: 107.99857 valid_loss: 455.39192 \n",
            "EarlyStopping counter: 1 out of 30\n",
            "\n",
            "----------\n",
            "Epoch: [ 40/300\n",
            "train_loss: 110.35141 valid_loss: 476.02376 \n",
            "EarlyStopping counter: 2 out of 30\n",
            "\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference**"
      ],
      "metadata": {
        "id": "GKFwLKKdiUcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft.eval() # prep model for evaluation\n",
        "targets, probs, preds =[], [], []\n",
        "for image_tensor, target in test_loader:  \n",
        "      #target = target.squeeze(1)     \n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "      \n",
        "      print(f\"target: {target}, pred: {output}\")"
      ],
      "metadata": {
        "id": "GKNbAjWmiQpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gsl1g8HUjGLs"
      }
    }
  ]
}