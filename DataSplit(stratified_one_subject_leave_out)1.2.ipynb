{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled90.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5vnMy8wYqRwKwDMznRxim",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CongenitalGlaucoma_AI_project/blob/main/DataSplit(stratified_one_subject_leave_out)1.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data_split for one-subject-leave-out stratified 5-fold crossvalidation**"
      ],
      "metadata": {
        "id": "Dxlpd0AbAWf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZREKDUM5uudx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Leave one subject out cross validation + 5-fold stratified cross validation\n",
        "\n",
        "・1症例を抜き出し、その症例のすべての画像をテスト画像とする\n",
        "・残りの症例の内斜視、外斜視、斜視なし群を、同じ症例が群をまたがないように5分割する。\n",
        "・5分割したデータセットのうち4つをtraining、1つをvalidationとして用いてトレーニングを行い、抜き出した1症例のそれぞれの画像のおける正解率を算出する。これを5回繰り返してcross validationとする。\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TkRaZnYjAjZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "nPSM5f-yyQfC",
        "outputId": "cd167314-1990-4b85-87a7-2ba63d483e9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "204\n",
            "633\n"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "\n",
        "gla_ortho_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\gla_ortho\"\n",
        "gla_eso_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\gla_eso\"\n",
        "gla_exo_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\gla_exo\"\n",
        "cont_ortho_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\cont_ortho\"\n",
        "cont_eso_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\cont_eso\"\n",
        "cont_exo_path = r\"F:\\先天性緑内障\\dataset_for_article_250px\\cont_exo\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "gla_ortho_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_d\"\n",
        "gla_eso_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_d__内斜視\"\n",
        "gla_exo_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_d__外斜視\"\n",
        "cont_ortho_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_control\"\n",
        "cont_eso_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_control__内斜視\\内斜視かぶりなし\"\n",
        "cont_exo_path = r\"F:\\先天性緑内障\\データ引継ぎ\\children_control__外斜視\\外斜視かぶりなし\"\n",
        "\"\"\"\n",
        "\n",
        "def make_path_list(dir):\n",
        "    path_list =  [file for file in glob.glob(dir+\"/*\") if os.path.isfile(file) == True ]\n",
        "    return path_list\n",
        "\n",
        "def extract_class(path_list, className):\n",
        "    class_list = list(itertools.repeat(className,len(path_list)))\n",
        "    return class_list\n",
        "\n",
        "def extract_ids(path_list):\n",
        "    id_list = [os.path.basename(name).split(\"_\")[0] for name in path_list]\n",
        "    return(id_list)\n",
        "\n",
        "\n",
        "gla_ortho_path_list = make_path_list(gla_ortho_path)\n",
        "gla_eso_path_list = make_path_list(gla_eso_path)\n",
        "gla_exo_path_list = make_path_list(gla_exo_path)\n",
        "cont_ortho_path_list = make_path_list(cont_ortho_path)\n",
        "cont_eso_path_list = make_path_list(cont_eso_path)\n",
        "cont_exo_path_list = make_path_list(cont_exo_path)\n",
        "\n",
        "#それぞれの項目（path, classes, ID）をリスト化\n",
        "gla_dataset_path = gla_ortho_path_list + gla_eso_path_list + gla_exo_path_list\n",
        "gla_classes = extract_class(gla_ortho_path_list, \"ortho\") + extract_class(gla_eso_path_list, \"eso\") + extract_class(gla_exo_path_list, \"exo\")\n",
        "gla_id = extract_ids(gla_ortho_path_list) + extract_ids(gla_eso_path_list) + extract_ids(gla_exo_path_list)\n",
        "cont_dataset_path = cont_ortho_path_list + cont_eso_path_list + cont_exo_path_list\n",
        "cont_classes = extract_class(cont_ortho_path_list, \"ortho\") + extract_class(cont_eso_path_list, \"eso\") + extract_class(cont_exo_path_list, \"exo\")\n",
        "cont_id = extract_ids(cont_ortho_path_list) + extract_ids(cont_eso_path_list) + extract_ids(cont_exo_path_list)\n",
        "\n",
        "#convert to Numpy\n",
        "gla_dataset_path = np.array(gla_dataset_path)\n",
        "gla_classes = np.array(gla_classes)\n",
        "gla_id = np.array(gla_id)\n",
        "cont_dataset_path = np.array(cont_dataset_path)\n",
        "cont_classes = np.array(cont_classes)\n",
        "cont_id = np.array(cont_id)\n",
        "\n",
        "print(len(gla_dataset_path))\n",
        "print(len(cont_dataset_path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "------test_dataset[0]\n",
        "  |\n",
        "  |---train_dataset_gla[0]----0\n",
        "  |                        |--1\n",
        "  |                        |--2\n",
        "  |                        |--3\n",
        "  |                        |--4\n",
        "  |---train_dataset_cont[0]----0\n",
        "  |                         |--1\n",
        "  |                         |--2\n",
        "  |                         |--3\n",
        "  |                         |--4\n",
        "  |---val_dataset_gla[0]----0\n",
        "  |                      |--1\n",
        "  |                      |--2\n",
        "  |                      |--3\n",
        "  |                      |--4\n",
        "  |---val_dataset_cont[0]----0\n",
        "  |                       |--1\n",
        "  |                       |--2\n",
        "  |                       |--3\n",
        "  |                       |--4\n",
        "  |---test_dataset[1]\n",
        "  ...\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cBs5SZf2m7Bj",
        "outputId": "e2c66e23-ff40-4044-cfc7-3d5d9fba9a50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n------test_dataset[0]\\n  |\\n  |---train_dataset_gla[0]----0\\n  |                        |--1\\n  |                        |--2\\n  |                        |--3\\n  |                        |--4\\n  |---train_dataset_cont[0]----0\\n  |                         |--1\\n  |                         |--2\\n  |                         |--3\\n  |                         |--4\\n  |---val_dataset_gla[0]----0\\n  |                      |--1\\n  |                      |--2\\n  |                      |--3\\n  |                      |--4\\n  |---val_dataset_cont[0]----0\\n  |                       |--1\\n  |                       |--2\\n  |                       |--3\\n  |                       |--4\\n  |---test_dataset[1]\\n  ...\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_gla, val_dataset_gla,train_dataset_cont, val_dataset_cont, test_dataset = [], [], [], [], []\n",
        "\n",
        "#まずglaのデータセットから1人分を抜き出す（LeaveOneGroupOut)\n",
        "# one group leave out 見本\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut\n",
        "# 今回のケースでは、groupがIDに該当\n",
        "logo = LeaveOneGroupOut()\n",
        "logo.get_n_splits(gla_dataset_path, gla_classes, gla_id)\n",
        "logo.get_n_splits(groups=gla_id)  # 'groups' is always required\n",
        "\n",
        "k=0\n",
        "for remain_index, test_index in logo.split(gla_dataset_path, gla_classes, gla_id):\n",
        "    gla_dataset_path_remain, gla_dataset_path_test = gla_dataset_path[remain_index], gla_dataset_path[test_index]\n",
        "    gla_classes_remain, gla_classes_test = gla_classes[remain_index], gla_classes[test_index]\n",
        "    gla_id_remain, gla_id_test = gla_id[remain_index], gla_id[test_index]\n",
        "    #print(gla_dataset_path, gla_dataset_path_test, gla_id_train, gla_id_test)\n",
        "    #print(\"test: \"+gla_id_test[0])\n",
        "    #print(\"TRAIN:\", remain_index, \"TEST:\", test_index)\n",
        "    #print(gla_dataset_path_test[0])\n",
        "    test_dataset.append(gla_dataset_path_test.tolist())\n",
        "\n",
        "    #抜き出した残りのglaについてStratified group 5-foldをかける\n",
        "    # example of stratified group Kfold　見本\n",
        "    # 今回のケースでは、groupがID、yがclassesに該当\n",
        "\n",
        "    cv = StratifiedGroupKFold(n_splits=5)\n",
        "    m=0 \n",
        "    train_miniset, val_miniset =  [0 for i in range(0, 5)], [0 for i in range(0, 5)]\n",
        "    for train_idxs, val_idxs in cv.split(gla_dataset_path_remain, gla_classes_remain, gla_id_remain):\n",
        "        #print(\"TRAIN:\", gla_classes_remain[train_idxs])\n",
        "        #print(\"      \", gla_id_remain[train_idxs])\n",
        "        #print(\"      \", gla_dataset_path_remain[train_idxs])\n",
        "        #print(\" TEST:\", gla_classes_remain[val_idxs])\n",
        "        #print(\"      \", gla_id_remain[val_idxs])\n",
        "        #print(\"      \", gla_dataset_path_remain[val_idxs])\n",
        "        train_miniset[m] = gla_dataset_path_remain[train_idxs].tolist()\n",
        "        val_miniset[m] = gla_dataset_path_remain[val_idxs].tolist()\n",
        "        m+=1\n",
        "    train_dataset_gla.append(train_miniset)\n",
        "    val_dataset_gla.append(val_miniset)\n",
        "    #print(\"train_dataset_added label[gla] \" + str(k))\n",
        "    k+=1\n",
        "\n",
        "    #controlについてStratified group 5-foldをかける\n",
        "    m=0 \n",
        "    train_miniset, val_miniset =  [0 for i in range(0, 5)], [0 for i in range(0, 5)]\n",
        "    for train_idxs, val_idxs in cv.split(cont_dataset_path, cont_classes, cont_id):\n",
        "        #print(\"TRAIN:\", cont_classes[train_idxs])\n",
        "        #print(\"      \", cont_id[train_idxs])\n",
        "        #print(\"      \", cont_dataset_path[train_idxs])\n",
        "        #print(\" TEST:\", cont_classes[val_idxs])\n",
        "        #print(\"      \", cont_id[val_idxs])\n",
        "        #print(\"      \", cont_dataset_path[val_idxs])\n",
        "        train_miniset[m] = cont_dataset_path[train_idxs].tolist()\n",
        "        val_miniset[m] = cont_dataset_path[val_idxs].tolist()\n",
        "        m+=1\n",
        "    train_dataset_cont.append(train_miniset)\n",
        "    val_dataset_cont.append(val_miniset)\n",
        "\n",
        "        \n",
        "#print(len(train_dataset_gla))    \n",
        "#print(len(val_dataset_gla))\n",
        "#print(val_dataset_gla)\n",
        "#print(len(train_dataset_cont))    \n",
        "#print(len(val_dataset_cont))\n",
        "#print(len(test_dataset))\n",
        "\n",
        "\n",
        "#同じくcontのデータセットから1人分抜き出してLeaveOneGroupOutをする\n",
        "logo = LeaveOneGroupOut()\n",
        "logo.get_n_splits(cont_dataset_path, cont_classes, cont_id)\n",
        "logo.get_n_splits(groups=cont_id)  # 'groups' is always required\n",
        "\n",
        "k=0\n",
        "for remain_index, test_index in logo.split(cont_dataset_path, cont_classes, cont_id):\n",
        "    cont_dataset_path_remain, cont_dataset_path_test = cont_dataset_path[remain_index], cont_dataset_path[test_index]\n",
        "    cont_classes_remain, cont_classes_test = cont_classes[remain_index], cont_classes[test_index]\n",
        "    cont_id_remain, cont_id_test = cont_id[remain_index], cont_id[test_index]\n",
        "    #print(cont_dataset_path_test[0])\n",
        "    test_dataset.append(cont_dataset_path_test.tolist())\n",
        "\n",
        "    #抜き出した残りのcontについてStratified group 5-foldをかける\n",
        "\n",
        "    cv = StratifiedGroupKFold(n_splits=5)\n",
        "    m=0 \n",
        "    train_miniset, val_miniset =  [0 for i in range(0, 5)], [0 for i in range(0, 5)]\n",
        "    for train_idxs, val_idxs in cv.split(cont_dataset_path_remain, cont_classes_remain, cont_id_remain):\n",
        "        train_miniset[m] = cont_dataset_path_remain[train_idxs].tolist()\n",
        "        val_miniset[m] = cont_dataset_path_remain[val_idxs].tolist()\n",
        "        m+=1\n",
        "    train_dataset_cont.append(train_miniset)\n",
        "    val_dataset_cont.append(val_miniset)\n",
        "\n",
        "    #glaについてStratified group 5-foldをかける\n",
        "    m=0 \n",
        "    train_miniset, val_miniset =  [0 for i in range(0, 5)], [0 for i in range(0, 5)]\n",
        "    for train_idxs, val_idxs in cv.split(gla_dataset_path, gla_classes, gla_id):\n",
        "        train_miniset[m] = gla_dataset_path[train_idxs].tolist()\n",
        "        val_miniset[m] = gla_dataset_path[val_idxs].tolist()\n",
        "        m+=1\n",
        "    train_dataset_gla.append(train_miniset)\n",
        "    val_dataset_gla.append(val_miniset)\n",
        "    #print(\"train_dataset_added label[cont] \"+ str(k))\n",
        "    k+=1\n",
        "        \n",
        "print(len(train_dataset_gla))    \n",
        "print(len(val_dataset_gla))\n",
        "print(len(train_dataset_cont))    \n",
        "print(len(val_dataset_cont))\n",
        "print(len(test_dataset))\n"
      ],
      "metadata": {
        "id": "XLe8wZOkQkkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6bc836-fb49-42ca-a066-5d9416d85532"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "660\n",
            "660\n",
            "660\n",
            "660\n",
            "660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pt=0\n",
        "fold=0\n",
        "train_list = train_dataset_gla[pt][fold] + train_dataset_cont[pt][fold]\n",
        "train_list_label = list(itertools.repeat(1, len(train_dataset_gla[pt][fold])))+list(itertools.repeat(0, len(train_dataset_cont[pt][fold])))\n",
        "val_list = val_dataset_gla[pt][fold] + val_dataset_cont[pt][fold]\n",
        "val_list_label = list(itertools.repeat(1, len(val_dataset_gla[pt][fold])))+list(itertools.repeat(0, len(val_dataset_cont[pt][fold])))\n",
        "test_list = test_dataset\n"
      ],
      "metadata": {
        "id": "7N0RU55Vircq"
      },
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])      \n",
        "        return tensor_image, target\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
      ],
      "metadata": {
        "id": "pFKghBCLiuK2"
      },
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 4, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 4, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle = False)"
      ],
      "metadata": {
        "id": "2pwoEIRBjGEx"
      },
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(val_loader))\n",
        "print(classes)\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "class_names = [\"cont\", \"gla\"]\n",
        "imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#水増し後の画像を可視化する関数\n",
        "def show_img(dataset):\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i in range(5):\n",
        "        image, label = dataset[i]\n",
        "        image = image.permute(1, 2, 0)\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.tick_params(labelbottom=False, labelleft=False, labelright=False, labeltop=False)\n",
        "        plt.tick_params(bottom=False, left=False, right=False, top=False)\n",
        "        plt.imshow(image)\n",
        "        print(label)\n",
        "\n",
        "#画像の可視化\n",
        "show_img(train_dataset)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_KUi82bNI2VL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}