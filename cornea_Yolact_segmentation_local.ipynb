{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwmrXUwaWtSVXiIlBL7Oqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/CongenitalGlaucoma_AI_project/blob/main/cornea_Yolact_segmentation_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Yolact_cornea_ellipse_segmentation**\n",
        "\n",
        "参考：https://farml1.com/yolact\n",
        "\n",
        "https://qiita.com/PoodleMaster/items/70cf03c1750fb58debb1"
      ],
      "metadata": {
        "id": "R6gvIhkwa-iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "glaucoma_cornea_ellipse_coco/\n",
        "├── annotations/\n",
        "│   ├── instances_default.json  # COCO形式のアノテーションファイル\n",
        "│\n",
        "└── images/\n",
        "    ├── 0-1.jpg  # 画像ファイル\n",
        "    ├── 1-0.jpg\n",
        "    ├── 2-1.jpg\n",
        "    ├── ...\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Xp5WO5xkcWwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUrzvRgWY42d",
        "outputId": "bca02546-f7fe-4c79-bf86-8421d890e48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.29.33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.5.5.62)\n",
            "Requirement already satisfied: pillow in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (9.0.0)\n",
            "Collecting pycocotools\n",
            "  Using cached pycocotools-2.0.6.tar.gz (24 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: matplotlib in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from opencv-python) (1.22.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (4.29.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (pyproject.toml): started\n",
            "  Building wheel for pycocotools (pyproject.toml): finished with status 'error'\n",
            "Failed to build pycocotools\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  Building wheel for pycocotools (pyproject.toml) did not run successfully.\n",
            "  exit code: 1\n",
            "  \n",
            "  [16 lines of output]\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build\\lib.win-amd64-cpython-38\n",
            "  creating build\\lib.win-amd64-cpython-38\\pycocotools\n",
            "  copying pycocotools\\coco.py -> build\\lib.win-amd64-cpython-38\\pycocotools\n",
            "  copying pycocotools\\cocoeval.py -> build\\lib.win-amd64-cpython-38\\pycocotools\n",
            "  copying pycocotools\\mask.py -> build\\lib.win-amd64-cpython-38\\pycocotools\n",
            "  copying pycocotools\\__init__.py -> build\\lib.win-amd64-cpython-38\\pycocotools\n",
            "  running build_ext\n",
            "  cythoning pycocotools/_mask.pyx to pycocotools\\_mask.c\n",
            "  building 'pycocotools._mask' extension\n",
            "  C:\\Users\\ykita\\AppData\\Local\\Temp\\pip-build-env-cdzsu1wq\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\ykita\\AppData\\Local\\Temp\\pip-install-gxabxtik\\pycocotools_6fe2d766bcbc4d0cb1154fd593057648\\pycocotools\\_mask.pyx\n",
            "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "  [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for pycocotools\n",
            "ERROR: Could not build wheels for pycocotools, which is required to install pyproject.toml-based projects\n",
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.8.2+cu110)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Requirement already satisfied: torch==1.7.1 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchvision) (1.7.1+cu110)\n",
            "Requirement already satisfied: numpy in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchvision) (1.22.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchvision) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch==1.7.1->torchvision) (4.0.1)\n",
            "Requirement already satisfied: torch in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.7.1+cu110)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (4.0.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\ykita\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (1.22.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install cython\n",
        "!pip install opencv-python pillow pycocotools matplotlib\n",
        "!pip install torchvision\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"F:\\先天性緑内障\"\n",
        "!git clone https://github.com/dbolya/yolact.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk0SKqlYaA5V",
        "outputId": "f6490768-35d3-4b50-88a7-6c2727b72a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F:\\先天性緑内障\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'yolact'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**学習のためフォルダを整理**"
      ],
      "metadata": {
        "id": "vSH3mFUCuTW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# 画像をtrainとvalに分ける\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# 画像フォルダと分割先のフォルダを指定\n",
        "image_folder = \"F:/先天性緑内障/glaucoma_cornea_ellipse_coco/images\"\n",
        "train_folder = \"F:/先天性緑内障/glaucoma_cornea_ellipse_coco_train/images/train\"\n",
        "val_folder = \"F:/先天性緑内障/glaucoma_cornea_ellipse_coco_train/images/val\"\n",
        "\n",
        "# フォルダを作成\n",
        "if os.path.exists(train_folder):\n",
        "    shutil.rmtree(train_folder) \n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "\n",
        "if os.path.exists(val_folder):\n",
        "    shutil.rmtree(val_folder) \n",
        "os.makedirs(val_folder, exist_ok=True)\n",
        "\n",
        "# 画像フォルダ内の画像ファイルを取得\n",
        "image_files = os.listdir(image_folder)\n",
        "random.Random(1).shuffle(image_files)  # ランダムシードを1に設定してシャッフル\n",
        "\n",
        "# 7:3に分割\n",
        "train_size = int(len(image_files) * 0.7)\n",
        "train_files = image_files[:train_size]\n",
        "val_files = image_files[train_size:]\n",
        "\n",
        "# 画像ファイルをコピー\n",
        "for file in train_files:\n",
        "    src = os.path.join(image_folder, file)\n",
        "    dst = os.path.join(train_folder, file)\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "for file in val_files:\n",
        "    src = os.path.join(image_folder, file)\n",
        "    dst = os.path.join(val_folder, file)\n",
        "    shutil.copy(src, dst)\n"
      ],
      "metadata": {
        "id": "anGAjUIHvkuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = f\"F:\\先天性緑内障\\glaucoma_cornea_ellipse_coco\"\n",
        "cocojson_path = f\"{dataset_path}/annotations/instances_default.json\"\n",
        "annotatedjson_path = f\"{dataset_path}/annotations/annotated.json\""
      ],
      "metadata": {
        "id": "XpCbEk6Tfplp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# 画像のfile_nameに基づいてjsonを分割\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# トレーニングデータとバリデーションデータの画像フォルダのパス\n",
        "train_image_dir = f\"F:/先天性緑内障/glaucoma_cornea_ellipse_coco_train/images/train\"\n",
        "val_image_dir = f\"F:/先天性緑内障\\/glaucoma_cornea_ellipse_coco_train/images/val\"\n",
        "\n",
        "# COCO JSONファイルのパス\n",
        "json_file_path = f\"F:/先天性緑内障/glaucoma_cornea_ellipse_coco/annotations/instances_default.json\"\n",
        "\n",
        "# 新しいJSONファイルの保存先\n",
        "train_json_path = f\"F:/先天性緑内障/glaucoma_cornea_ellipse_coco_train/annotations/train.json\"\n",
        "val_json_path = f\"F:/先天性緑内障/glaucoma_cornea_ellipse_coco_train/annotations/val.json\"\n",
        "\n",
        "# COCO JSONファイルを読み込む\n",
        "with open(json_file_path, \"r\") as f:\n",
        "    coco_json = json.load(f)\n",
        "\n",
        "# トレーニングデータとバリデーションデータに分割する\n",
        "train_data = {\"images\": [], \"annotations\": [], \"categories\": coco_json[\"categories\"]}\n",
        "val_data = {\"images\": [], \"annotations\": [], \"categories\": coco_json[\"categories\"]}\n",
        "\n",
        "for image in coco_json[\"images\"]:\n",
        "    if os.path.exists(os.path.join(train_image_dir, image[\"file_name\"])):\n",
        "        train_data[\"images\"].append(image)\n",
        "    elif os.path.exists(os.path.join(val_image_dir, image[\"file_name\"])):\n",
        "        val_data[\"images\"].append(image)\n",
        "\n",
        "for annotation in coco_json[\"annotations\"]:\n",
        "    image_id = annotation[\"image_id\"]\n",
        "    if image_id in [image[\"id\"] for image in train_data[\"images\"]]:\n",
        "        train_data[\"annotations\"].append(annotation)\n",
        "    elif image_id in [image[\"id\"] for image in val_data[\"images\"]]:\n",
        "        val_data[\"annotations\"].append(annotation)\n",
        "\n",
        "# トレーニングデータとバリデーションデータのJSONファイルを保存する\n",
        "with open(train_json_path, \"w\") as f:\n",
        "    json.dump(train_data, f)\n",
        "\n",
        "with open(val_json_path, \"w\") as f:\n",
        "    json.dump(val_data, f)\n"
      ],
      "metadata": {
        "id": "tqycdgOU4P1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://farml1.com/yolact/\n",
        "# https://qiita.com/PoodleMaster/items/70cf03c1750fb58debb1\n",
        "\n",
        "# yolact > data > config.pyの --DATASETS--の最後に以下を追加\n",
        "\n",
        "\"\"\"\n",
        "my_custom_dataset = dataset_base.copy({\n",
        "    'name': 'My Dataset',\n",
        "\n",
        "    'train_images': f\"F:/先天性緑内障/glaucoma_cornea_ellipse_coco_train/images/train\",\n",
        "    'train_info':   f\"F:/先天性緑内障/glaucoma_cornea_ellipse_coco_train/annotations/train.json\"\n",
        "\n",
        "    'valid_images': f\"F:/先天性緑内障/glaucoma_cornea_ellipse_coco_train/images/val\",\n",
        "    'valid_info':   f\"F:/先天性緑内障/glaucoma_cornea_ellipse_coco_train/annotations/val.json\",\n",
        "\n",
        "    'has_gt': True,\n",
        "\n",
        "    # jsonの[categories]-[name]を参照して設定すること\n",
        "    'class_names': ('corneal margin')\n",
        "})\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#yolact > data > config.pyの --YOLACT v1.0 CONFIGS-- を以下に示すようにcoco2017をmy_customへ書き換え\n",
        "\n",
        "\"\"\"\n",
        "    # Dataset stuff\n",
        "    'dataset': coco2017_dataset,\n",
        "    'num_classes': len(coco2017_dataset.class_names) + 1,\n",
        "    ↓\n",
        "    # Dataset stuff\n",
        "    'dataset': my_custom_dataset,\n",
        "    'num_classes': len(my_custom_dataset.class_names) + 1,\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRxQMbkvxui8",
        "outputId": "b5fcde96-1f26-4dca-eb04-f51e18422140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    # Dataset stuff\\n    'dataset': coco2017_dataset,\\n    'num_classes': len(coco2017_dataset.class_names) + 1,\\n    ↓\\n    # Dataset stuff\\n    'dataset': my_custom_dataset,\\n    'num_classes': len(my_custom_dataset.class_names) + 1,\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "YOLACTのGithubの中段Evaluationの表にある「yolact_base_54_800000.pth」をダウンロード\n",
        "ダウンロードした事前学習済みデータは、yolact/weightsディレクトリ下へ格納\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nDT4Pf0QMrNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolact_path = f\"F:/先天性緑内障/yolact\"\n",
        "\n",
        "!cd $yolact_path #ご自身の環境で作業ディレクトリへ移動してください。\n",
        "!python train.py --config yolact_base_config --resume weights/yolact_base_54_800000.pth --start_iter 0 --batch_size 8 --num_workers 0 --validation_epoch -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9j1cP6kBkPs",
        "outputId": "cd8e7664-f449-439b-c2cf-cf46f70506a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "指定されたパスが見つかりません。\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 1, in <module>\n",
            "    from data import *\n",
            "  File \"F:\\先天性緑内障\\yolact\\data\\__init__.py\", line 2, in <module>\n",
            "    from .coco import *\n",
            "  File \"F:\\先天性緑内障\\yolact\\data\\coco.py\", line 10, in <module>\n",
            "    from pycocotools import mask as maskUtils\n",
            "ModuleNotFoundError: No module named 'pycocotools'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ここからはcolabのインスタンスを使う（エラーが出てうまくいかないため）**\n",
        "\n",
        "https://qiita.com/PoodleMaster/items/d0bf9cac77e5af61c972"
      ],
      "metadata": {
        "id": "kQ7YdnZBTI9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "#GDriveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeTwX9S1TUNu",
        "outputId": "9af37d09-89a5-481c-ba99-b1216f7077fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cython --q\n",
        "!pip install opencv-python pillow pycocotools matplotlib\n",
        "!pip install torchvision==0.5.0\n",
        "!pip install torch==1.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re2WhRKRUMdm",
        "outputId": "2a870d15-9700-4608-ca79-57d662081eb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (2.0.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchvision==0.5.0\n",
            "  Downloading torchvision-0.5.0-cp38-cp38-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp38-cp38-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.4/753.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.5.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.5.0) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n",
            "fastai 2.7.11 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "fastai 2.7.11 requires torchvision>=0.8.2, but you have torchvision 0.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.4.0 torchvision-0.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.8/dist-packages (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/dbolya/yolact.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j-Pbt4ZUfv-",
        "outputId": "44513774-03f5-4b75-bb4c-1ddf8f687dbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'yolact'...\n",
            "remote: Enumerating objects: 2936, done.\u001b[K\n",
            "remote: Total 2936 (delta 0), reused 0 (delta 0), pack-reused 2936\u001b[K\n",
            "Receiving objects: 100% (2936/2936), 21.20 MiB | 10.49 MiB/s, done.\n",
            "Resolving deltas: 100% (2002/2002), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Yolact++を使いたいとき\n",
        "%cd /content/yolact/external/DCNv2\n",
        "!python setup.py build develop"
      ],
      "metadata": {
        "id": "dHg2LEnJUkt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained modelをダウンロード\n",
        "import os\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/chentinghao/download_google_drive.git\n",
        "os.makedirs(\"/content/yolact/weights\", exist_ok =True)\n",
        "#!mkdir -p /content/weights\n",
        "!python ./download_google_drive/download_gdrive.py 1tvqFPd4bJtakOlmn-uIA492g2qurRChj /content/yolact/weights/resnet101_reducedfc.pth\n",
        "\n",
        "# os.makedirs(\"/content/yolact/weights\", exist_ok =True)\n",
        "# shutil.copy(\"/content/drive/MyDrive/Deep_learning/Congenital_Glaucoma/glaucoma_cornea_ellipse_coco_train/yolact_base_54_800000.pth\", \"/content/yolact/weights/yolact_base_54_800000.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wROAHx9QaEe9",
        "outputId": "e866f662-d6ff-4d09-c97f-c63f8a404fbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'download_google_drive'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Total 16 (delta 0), reused 0 (delta 0), pack-reused 16\u001b[K\n",
            "Unpacking objects: 100% (16/16), 4.77 KiB | 1.19 MiB/s, done.\n",
            "./download_google_drive/download_gdrive.py:50: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  if len(sys.argv) is not 3:\n",
            "32.0kB [00:00, 70.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## config.py をGdrive上のパスに書き換え"
      ],
      "metadata": {
        "id": "luq3rgXBWoSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/yolact/data/config.py\n",
        "from backbone import ResNetBackbone, VGGBackbone, ResNetBackboneGN, DarkNetBackbone\n",
        "from math import sqrt\n",
        "import torch\n",
        "\n",
        "# for making bounding boxes pretty\n",
        "COLORS = ((244,  67,  54),\n",
        "          (233,  30,  99),\n",
        "          (156,  39, 176),\n",
        "          (103,  58, 183),\n",
        "          ( 63,  81, 181),\n",
        "          ( 33, 150, 243),\n",
        "          (  3, 169, 244),\n",
        "          (  0, 188, 212),\n",
        "          (  0, 150, 136),\n",
        "          ( 76, 175,  80),\n",
        "          (139, 195,  74),\n",
        "          (205, 220,  57),\n",
        "          (255, 235,  59),\n",
        "          (255, 193,   7),\n",
        "          (255, 152,   0),\n",
        "          (255,  87,  34),\n",
        "          (121,  85,  72),\n",
        "          (158, 158, 158),\n",
        "          ( 96, 125, 139))\n",
        "\n",
        "\n",
        "# These are in BGR and are for ImageNet\n",
        "MEANS = (103.94, 116.78, 123.68)\n",
        "STD   = (57.38, 57.12, 58.40)\n",
        "\n",
        "COCO_CLASSES = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "                'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
        "                'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
        "                'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',\n",
        "                'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "                'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
        "                'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "                'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "                'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n",
        "                'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',\n",
        "                'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',\n",
        "                'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
        "                'scissors', 'teddy bear', 'hair drier', 'toothbrush')\n",
        "\n",
        "COCO_LABEL_MAP = { 1:  1,  2:  2,  3:  3,  4:  4,  5:  5,  6:  6,  7:  7,  8:  8,\n",
        "                   9:  9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16,\n",
        "                  18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24,\n",
        "                  27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32,\n",
        "                  37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40,\n",
        "                  46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48,\n",
        "                  54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56,\n",
        "                  62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64,\n",
        "                  74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72,\n",
        "                  82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- CONFIG CLASS ----------------------- #\n",
        "\n",
        "class Config(object):\n",
        "    \"\"\"\n",
        "    Holds the configuration for anything you want it to.\n",
        "    To get the currently active config, call get_cfg().\n",
        "\n",
        "    To use, just do cfg.x instead of cfg['x'].\n",
        "    I made this because doing cfg['x'] all the time is dumb.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_dict):\n",
        "        for key, val in config_dict.items():\n",
        "            self.__setattr__(key, val)\n",
        "\n",
        "    def copy(self, new_config_dict={}):\n",
        "        \"\"\"\n",
        "        Copies this config into a new config object, making\n",
        "        the changes given by new_config_dict.\n",
        "        \"\"\"\n",
        "\n",
        "        ret = Config(vars(self))\n",
        "        \n",
        "        for key, val in new_config_dict.items():\n",
        "            ret.__setattr__(key, val)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def replace(self, new_config_dict):\n",
        "        \"\"\"\n",
        "        Copies new_config_dict into this config object.\n",
        "        Note: new_config_dict can also be a config object.\n",
        "        \"\"\"\n",
        "        if isinstance(new_config_dict, Config):\n",
        "            new_config_dict = vars(new_config_dict)\n",
        "\n",
        "        for key, val in new_config_dict.items():\n",
        "            self.__setattr__(key, val)\n",
        "    \n",
        "    def print(self):\n",
        "        for k, v in vars(self).items():\n",
        "            print(k, ' = ', v)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- DATASETS ----------------------- #\n",
        "\n",
        "dataset_base = Config({\n",
        "    'name': 'Base Dataset',\n",
        "\n",
        "    # Training images and annotations\n",
        "    'train_images': './data/coco/images/',\n",
        "    'train_info':   'path_to_annotation_file',\n",
        "\n",
        "    # Validation images and annotations.\n",
        "    'valid_images': './data/coco/images/',\n",
        "    'valid_info':   'path_to_annotation_file',\n",
        "\n",
        "    # Whether or not to load GT. If this is False, eval.py quantitative evaluation won't work.\n",
        "    'has_gt': True,\n",
        "\n",
        "    # A list of names for each of you classes.\n",
        "    'class_names': COCO_CLASSES,\n",
        "\n",
        "    # COCO class ids aren't sequential, so this is a bandage fix. If your ids aren't sequential,\n",
        "    # provide a map from category_id -> index in class_names + 1 (the +1 is there because it's 1-indexed).\n",
        "    # If not specified, this just assumes category ids start at 1 and increase sequentially.\n",
        "    'label_map': None\n",
        "})\n",
        "\n",
        "coco2014_dataset = dataset_base.copy({\n",
        "    'name': 'COCO 2014',\n",
        "    \n",
        "    'train_info': './data/coco/annotations/instances_train2014.json',\n",
        "    'valid_info': './data/coco/annotations/instances_val2014.json',\n",
        "\n",
        "    'label_map': COCO_LABEL_MAP\n",
        "})\n",
        "\n",
        "coco2017_dataset = dataset_base.copy({\n",
        "    'name': 'COCO 2017',\n",
        "    \n",
        "    'train_info': './data/coco/annotations/instances_train2017.json',\n",
        "    'valid_info': './data/coco/annotations/instances_val2017.json',\n",
        "\n",
        "    'label_map': COCO_LABEL_MAP\n",
        "})\n",
        "\n",
        "coco2017_testdev_dataset = dataset_base.copy({\n",
        "    'name': 'COCO 2017 Test-Dev',\n",
        "\n",
        "    'valid_info': './data/coco/annotations/image_info_test-dev2017.json',\n",
        "    'has_gt': False,\n",
        "\n",
        "    'label_map': COCO_LABEL_MAP\n",
        "})\n",
        "\n",
        "PASCAL_CLASSES = (\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
        "                  \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
        "                  \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\n",
        "                  \"sheep\", \"sofa\", \"train\", \"tvmonitor\")\n",
        "\n",
        "pascal_sbd_dataset = dataset_base.copy({\n",
        "    'name': 'Pascal SBD 2012',\n",
        "\n",
        "    'train_images': './data/sbd/img',\n",
        "    'valid_images': './data/sbd/img',\n",
        "    \n",
        "    'train_info': './data/sbd/pascal_sbd_train.json',\n",
        "    'valid_info': './data/sbd/pascal_sbd_val.json',\n",
        "\n",
        "    'class_names': PASCAL_CLASSES,\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "my_custom_dataset = dataset_base.copy({\n",
        "    'name': 'My Dataset',\n",
        "\n",
        "    'train_images': \"/content/drive/MyDrive/Deep_learning/Congenital_Glaucoma/glaucoma_cornea_ellipse_coco_train/images/train\",\n",
        "    'train_info':   \"/content/drive/MyDrive/Deep_learning/Congenital_Glaucoma/glaucoma_cornea_ellipse_coco_train/annotations/train.json\",\n",
        "\n",
        "    'valid_images': \"/content/drive/MyDrive/Deep_learning/Congenital_Glaucoma/glaucoma_cornea_ellipse_coco_train/images/val\",\n",
        "    'valid_info':   \"/content/drive/MyDrive/Deep_learning/Congenital_Glaucoma/glaucoma_cornea_ellipse_coco_train/annotations/val.json\",\n",
        "\n",
        "    'has_gt': True,\n",
        "\n",
        "    # jsonの[categories]-[name]を参照して設定すること\n",
        "    'class_names': ('corneal margin'),\n",
        "\n",
        "    # ラベルマップ\n",
        "    # jsonの[categories]-[id]を参照して設定すること\n",
        "    'label_map'   : { 1:1 },\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- TRANSFORMS ----------------------- #\n",
        "\n",
        "resnet_transform = Config({\n",
        "    'channel_order': 'RGB',\n",
        "    'normalize': True,\n",
        "    'subtract_means': False,\n",
        "    'to_float': False,\n",
        "})\n",
        "\n",
        "vgg_transform = Config({\n",
        "    # Note that though vgg is traditionally BGR,\n",
        "    # the channel order of vgg_reducedfc.pth is RGB.\n",
        "    'channel_order': 'RGB',\n",
        "    'normalize': False,\n",
        "    'subtract_means': True,\n",
        "    'to_float': False,\n",
        "})\n",
        "\n",
        "darknet_transform = Config({\n",
        "    'channel_order': 'RGB',\n",
        "    'normalize': False,\n",
        "    'subtract_means': False,\n",
        "    'to_float': True,\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- BACKBONES ----------------------- #\n",
        "\n",
        "backbone_base = Config({\n",
        "    'name': 'Base Backbone',\n",
        "    'path': 'path/to/pretrained/weights',\n",
        "    'type': object,\n",
        "    'args': tuple(),\n",
        "    'transform': resnet_transform,\n",
        "\n",
        "    'selected_layers': list(),\n",
        "    'pred_scales': list(),\n",
        "    'pred_aspect_ratios': list(),\n",
        "\n",
        "    'use_pixel_scales': False,\n",
        "    'preapply_sqrt': True,\n",
        "    'use_square_anchors': False,\n",
        "})\n",
        "\n",
        "resnet101_backbone = backbone_base.copy({\n",
        "    'name': 'ResNet101',\n",
        "    'path': 'resnet101_reducedfc.pth',\n",
        "    'type': ResNetBackbone,\n",
        "    'args': ([3, 4, 23, 3],),\n",
        "    'transform': resnet_transform,\n",
        "\n",
        "    'selected_layers': list(range(2, 8)),\n",
        "    'pred_scales': [[1]]*6,\n",
        "    'pred_aspect_ratios': [ [[0.66685089, 1.7073535, 0.87508774, 1.16524493, 0.49059086]] ] * 6,\n",
        "})\n",
        "\n",
        "resnet101_gn_backbone = backbone_base.copy({\n",
        "    'name': 'ResNet101_GN',\n",
        "    'path': 'R-101-GN.pkl',\n",
        "    'type': ResNetBackboneGN,\n",
        "    'args': ([3, 4, 23, 3],),\n",
        "    'transform': resnet_transform,\n",
        "\n",
        "    'selected_layers': list(range(2, 8)),\n",
        "    'pred_scales': [[1]]*6,\n",
        "    'pred_aspect_ratios': [ [[0.66685089, 1.7073535, 0.87508774, 1.16524493, 0.49059086]] ] * 6,\n",
        "})\n",
        "\n",
        "resnet101_dcn_inter3_backbone = resnet101_backbone.copy({\n",
        "    'name': 'ResNet101_DCN_Interval3',\n",
        "    'args': ([3, 4, 23, 3], [0, 4, 23, 3], 3),\n",
        "})\n",
        "\n",
        "resnet50_backbone = resnet101_backbone.copy({\n",
        "    'name': 'ResNet50',\n",
        "    'path': 'resnet50-19c8e357.pth',\n",
        "    'type': ResNetBackbone,\n",
        "    'args': ([3, 4, 6, 3],),\n",
        "    'transform': resnet_transform,\n",
        "})\n",
        "\n",
        "resnet50_dcnv2_backbone = resnet50_backbone.copy({\n",
        "    'name': 'ResNet50_DCNv2',\n",
        "    'args': ([3, 4, 6, 3], [0, 4, 6, 3]),\n",
        "})\n",
        "\n",
        "darknet53_backbone = backbone_base.copy({\n",
        "    'name': 'DarkNet53',\n",
        "    'path': 'darknet53.pth',\n",
        "    'type': DarkNetBackbone,\n",
        "    'args': ([1, 2, 8, 8, 4],),\n",
        "    'transform': darknet_transform,\n",
        "\n",
        "    'selected_layers': list(range(3, 9)),\n",
        "    'pred_scales': [[3.5, 4.95], [3.6, 4.90], [3.3, 4.02], [2.7, 3.10], [2.1, 2.37], [1.8, 1.92]],\n",
        "    'pred_aspect_ratios': [ [[1, sqrt(2), 1/sqrt(2), sqrt(3), 1/sqrt(3)][:n], [1]] for n in [3, 5, 5, 5, 3, 3] ],\n",
        "})\n",
        "\n",
        "vgg16_arch = [[64, 64],\n",
        "              [ 'M', 128, 128],\n",
        "              [ 'M', 256, 256, 256],\n",
        "              [('M', {'kernel_size': 2, 'stride': 2, 'ceil_mode': True}), 512, 512, 512],\n",
        "              [ 'M', 512, 512, 512],\n",
        "              [('M',  {'kernel_size': 3, 'stride':  1, 'padding':  1}),\n",
        "               (1024, {'kernel_size': 3, 'padding': 6, 'dilation': 6}),\n",
        "               (1024, {'kernel_size': 1})]]\n",
        "\n",
        "vgg16_backbone = backbone_base.copy({\n",
        "    'name': 'VGG16',\n",
        "    'path': 'vgg16_reducedfc.pth',\n",
        "    'type': VGGBackbone,\n",
        "    'args': (vgg16_arch, [(256, 2), (128, 2), (128, 1), (128, 1)], [3]),\n",
        "    'transform': vgg_transform,\n",
        "\n",
        "    'selected_layers': [3] + list(range(5, 10)),\n",
        "    'pred_scales': [[5, 4]]*6,\n",
        "    'pred_aspect_ratios': [ [[1], [1, sqrt(2), 1/sqrt(2), sqrt(3), 1/sqrt(3)][:n]] for n in [3, 5, 5, 5, 3, 3] ],\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- MASK BRANCH TYPES ----------------------- #\n",
        "\n",
        "mask_type = Config({\n",
        "    # Direct produces masks directly as the output of each pred module.\n",
        "    # This is denoted as fc-mask in the paper.\n",
        "    # Parameters: mask_size, use_gt_bboxes\n",
        "    'direct': 0,\n",
        "\n",
        "    # Lincomb produces coefficients as the output of each pred module then uses those coefficients\n",
        "    # to linearly combine features from a prototype network to create image-sized masks.\n",
        "    # Parameters:\n",
        "    #   - masks_to_train (int): Since we're producing (near) full image masks, it'd take too much\n",
        "    #                           vram to backprop on every single mask. Thus we select only a subset.\n",
        "    #   - mask_proto_src (int): The input layer to the mask prototype generation network. This is an\n",
        "    #                           index in backbone.layers. Use to use the image itself instead.\n",
        "    #   - mask_proto_net (list<tuple>): A list of layers in the mask proto network with the last one\n",
        "    #                                   being where the masks are taken from. Each conv layer is in\n",
        "    #                                   the form (num_features, kernel_size, **kwdargs). An empty\n",
        "    #                                   list means to use the source for prototype masks. If the\n",
        "    #                                   kernel_size is negative, this creates a deconv layer instead.\n",
        "    #                                   If the kernel_size is negative and the num_features is None,\n",
        "    #                                   this creates a simple bilinear interpolation layer instead.\n",
        "    #   - mask_proto_bias (bool): Whether to include an extra coefficient that corresponds to a proto\n",
        "    #                             mask of all ones.\n",
        "    #   - mask_proto_prototype_activation (func): The activation to apply to each prototype mask.\n",
        "    #   - mask_proto_mask_activation (func): After summing the prototype masks with the predicted\n",
        "    #                                        coeffs, what activation to apply to the final mask.\n",
        "    #   - mask_proto_coeff_activation (func): The activation to apply to the mask coefficients.\n",
        "    #   - mask_proto_crop (bool): If True, crop the mask with the predicted bbox during training.\n",
        "    #   - mask_proto_crop_expand (float): If cropping, the percent to expand the cropping bbox by\n",
        "    #                                     in each direction. This is to make the model less reliant\n",
        "    #                                     on perfect bbox predictions.\n",
        "    #   - mask_proto_loss (str [l1|disj]): If not None, apply an l1 or disjunctive regularization\n",
        "    #                                      loss directly to the prototype masks.\n",
        "    #   - mask_proto_binarize_downsampled_gt (bool): Binarize GT after dowsnampling during training?\n",
        "    #   - mask_proto_normalize_mask_loss_by_sqrt_area (bool): Whether to normalize mask loss by sqrt(sum(gt))\n",
        "    #   - mask_proto_reweight_mask_loss (bool): Reweight mask loss such that background is divided by\n",
        "    #                                           #background and foreground is divided by #foreground.\n",
        "    #   - mask_proto_grid_file (str): The path to the grid file to use with the next option.\n",
        "    #                                 This should be a numpy.dump file with shape [numgrids, h, w]\n",
        "    #                                 where h and w are w.r.t. the mask_proto_src convout.\n",
        "    #   - mask_proto_use_grid (bool): Whether to add extra grid features to the proto_net input.\n",
        "    #   - mask_proto_coeff_gate (bool): Add an extra set of sigmoided coefficients that is multiplied\n",
        "    #                                   into the predicted coefficients in order to \"gate\" them.\n",
        "    #   - mask_proto_prototypes_as_features (bool): For each prediction module, downsample the prototypes\n",
        "    #                                 to the convout size of that module and supply the prototypes as input\n",
        "    #                                 in addition to the already supplied backbone features.\n",
        "    #   - mask_proto_prototypes_as_features_no_grad (bool): If the above is set, don't backprop gradients to\n",
        "    #                                 to the prototypes from the network head.\n",
        "    #   - mask_proto_remove_empty_masks (bool): Remove masks that are downsampled to 0 during loss calculations.\n",
        "    #   - mask_proto_reweight_coeff (float): The coefficient to multiple the forground pixels with if reweighting.\n",
        "    #   - mask_proto_coeff_diversity_loss (bool): Apply coefficient diversity loss on the coefficients so that the same\n",
        "    #                                             instance has similar coefficients.\n",
        "    #   - mask_proto_coeff_diversity_alpha (float): The weight to use for the coefficient diversity loss.\n",
        "    #   - mask_proto_normalize_emulate_roi_pooling (bool): Normalize the mask loss to emulate roi pooling's affect on loss.\n",
        "    #   - mask_proto_double_loss (bool): Whether to use the old loss in addition to any special new losses.\n",
        "    #   - mask_proto_double_loss_alpha (float): The alpha to weight the above loss.\n",
        "    #   - mask_proto_split_prototypes_by_head (bool): If true, this will give each prediction head its own prototypes.\n",
        "    #   - mask_proto_crop_with_pred_box (bool): Whether to crop with the predicted box or the gt box.\n",
        "    'lincomb': 1,\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- ACTIVATION FUNCTIONS ----------------------- #\n",
        "\n",
        "activation_func = Config({\n",
        "    'tanh':    torch.tanh,\n",
        "    'sigmoid': torch.sigmoid,\n",
        "    'softmax': lambda x: torch.nn.functional.softmax(x, dim=-1),\n",
        "    'relu':    lambda x: torch.nn.functional.relu(x, inplace=True),\n",
        "    'none':    lambda x: x,\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- FPN DEFAULTS ----------------------- #\n",
        "\n",
        "fpn_base = Config({\n",
        "    # The number of features to have in each FPN layer\n",
        "    'num_features': 256,\n",
        "\n",
        "    # The upsampling mode used\n",
        "    'interpolation_mode': 'bilinear',\n",
        "\n",
        "    # The number of extra layers to be produced by downsampling starting at P5\n",
        "    'num_downsample': 1,\n",
        "\n",
        "    # Whether to down sample with a 3x3 stride 2 conv layer instead of just a stride 2 selection\n",
        "    'use_conv_downsample': False,\n",
        "\n",
        "    # Whether to pad the pred layers with 1 on each side (I forgot to add this at the start)\n",
        "    # This is just here for backwards compatibility\n",
        "    'pad': True,\n",
        "\n",
        "    # Whether to add relu to the downsampled layers.\n",
        "    'relu_downsample_layers': False,\n",
        "\n",
        "    # Whether to add relu to the regular layers\n",
        "    'relu_pred_layers': True,\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- CONFIG DEFAULTS ----------------------- #\n",
        "\n",
        "coco_base_config = Config({\n",
        "    'dataset': coco2014_dataset,\n",
        "    'num_classes': 81, # This should include the background class\n",
        "\n",
        "    'max_iter': 400000,\n",
        "\n",
        "    # The maximum number of detections for evaluation\n",
        "    'max_num_detections': 100,\n",
        "\n",
        "    # dw' = momentum * dw - lr * (grad + decay * w)\n",
        "    'lr': 1e-3,\n",
        "    'momentum': 0.9,\n",
        "    'decay': 5e-4,\n",
        "\n",
        "    # For each lr step, what to multiply the lr with\n",
        "    'gamma': 0.1,\n",
        "    'lr_steps': (280000, 360000, 400000),\n",
        "\n",
        "    # Initial learning rate to linearly warmup from (if until > 0)\n",
        "    'lr_warmup_init': 1e-4,\n",
        "\n",
        "    # If > 0 then increase the lr linearly from warmup_init to lr each iter for until iters\n",
        "    'lr_warmup_until': 500,\n",
        "\n",
        "    # The terms to scale the respective loss by\n",
        "    'conf_alpha': 1,\n",
        "    'bbox_alpha': 1.5,\n",
        "    'mask_alpha': 0.4 / 256 * 140 * 140, # Some funky equation. Don't worry about it.\n",
        "\n",
        "    # Eval.py sets this if you just want to run YOLACT as a detector\n",
        "    'eval_mask_branch': True,\n",
        "\n",
        "    # Top_k examples to consider for NMS\n",
        "    'nms_top_k': 200,\n",
        "    # Examples with confidence less than this are not considered by NMS\n",
        "    'nms_conf_thresh': 0.05,\n",
        "    # Boxes with IoU overlap greater than this threshold will be culled during NMS\n",
        "    'nms_thresh': 0.5,\n",
        "\n",
        "    # See mask_type for details.\n",
        "    'mask_type': mask_type.direct,\n",
        "    'mask_size': 16,\n",
        "    'masks_to_train': 100,\n",
        "    'mask_proto_src': None,\n",
        "    'mask_proto_net': [(256, 3, {}), (256, 3, {})],\n",
        "    'mask_proto_bias': False,\n",
        "    'mask_proto_prototype_activation': activation_func.relu,\n",
        "    'mask_proto_mask_activation': activation_func.sigmoid,\n",
        "    'mask_proto_coeff_activation': activation_func.tanh,\n",
        "    'mask_proto_crop': True,\n",
        "    'mask_proto_crop_expand': 0,\n",
        "    'mask_proto_loss': None,\n",
        "    'mask_proto_binarize_downsampled_gt': True,\n",
        "    'mask_proto_normalize_mask_loss_by_sqrt_area': False,\n",
        "    'mask_proto_reweight_mask_loss': False,\n",
        "    'mask_proto_grid_file': 'data/grid.npy',\n",
        "    'mask_proto_use_grid':  False,\n",
        "    'mask_proto_coeff_gate': False,\n",
        "    'mask_proto_prototypes_as_features': False,\n",
        "    'mask_proto_prototypes_as_features_no_grad': False,\n",
        "    'mask_proto_remove_empty_masks': False,\n",
        "    'mask_proto_reweight_coeff': 1,\n",
        "    'mask_proto_coeff_diversity_loss': False,\n",
        "    'mask_proto_coeff_diversity_alpha': 1,\n",
        "    'mask_proto_normalize_emulate_roi_pooling': False,\n",
        "    'mask_proto_double_loss': False,\n",
        "    'mask_proto_double_loss_alpha': 1,\n",
        "    'mask_proto_split_prototypes_by_head': False,\n",
        "    'mask_proto_crop_with_pred_box': False,\n",
        "\n",
        "    # SSD data augmentation parameters\n",
        "    # Randomize hue, vibrance, etc.\n",
        "    'augment_photometric_distort': True,\n",
        "    # Have a chance to scale down the image and pad (to emulate smaller detections)\n",
        "    'augment_expand': True,\n",
        "    # Potentialy sample a random crop from the image and put it in a random place\n",
        "    'augment_random_sample_crop': True,\n",
        "    # Mirror the image with a probability of 1/2\n",
        "    'augment_random_mirror': True,\n",
        "    # Flip the image vertically with a probability of 1/2\n",
        "    'augment_random_flip': False,\n",
        "    # With uniform probability, rotate the image [0,90,180,270] degrees\n",
        "    'augment_random_rot90': False,\n",
        "\n",
        "    # Discard detections with width and height smaller than this (in absolute width and height)\n",
        "    'discard_box_width': 4 / 550,\n",
        "    'discard_box_height': 4 / 550,\n",
        "\n",
        "    # If using batchnorm anywhere in the backbone, freeze the batchnorm layer during training.\n",
        "    # Note: any additional batch norm layers after the backbone will not be frozen.\n",
        "    'freeze_bn': False,\n",
        "\n",
        "    # Set this to a config object if you want an FPN (inherit from fpn_base). See fpn_base for details.\n",
        "    'fpn': None,\n",
        "\n",
        "    # Use the same weights for each network head\n",
        "    'share_prediction_module': False,\n",
        "\n",
        "    # For hard negative mining, instead of using the negatives that are leastl confidently background,\n",
        "    # use negatives that are most confidently not background.\n",
        "    'ohem_use_most_confident': False,\n",
        "\n",
        "    # Use focal loss as described in https://arxiv.org/pdf/1708.02002.pdf instead of OHEM\n",
        "    'use_focal_loss': False,\n",
        "    'focal_loss_alpha': 0.25,\n",
        "    'focal_loss_gamma': 2,\n",
        "    \n",
        "    # The initial bias toward forground objects, as specified in the focal loss paper\n",
        "    'focal_loss_init_pi': 0.01,\n",
        "\n",
        "    # Keeps track of the average number of examples for each class, and weights the loss for that class accordingly.\n",
        "    'use_class_balanced_conf': False,\n",
        "\n",
        "    # Whether to use sigmoid focal loss instead of softmax, all else being the same.\n",
        "    'use_sigmoid_focal_loss': False,\n",
        "\n",
        "    # Use class[0] to be the objectness score and class[1:] to be the softmax predicted class.\n",
        "    # Note: at the moment this is only implemented if use_focal_loss is on.\n",
        "    'use_objectness_score': False,\n",
        "\n",
        "    # Adds a global pool + fc layer to the smallest selected layer that predicts the existence of each of the 80 classes.\n",
        "    # This branch is only evaluated during training time and is just there for multitask learning.\n",
        "    'use_class_existence_loss': False,\n",
        "    'class_existence_alpha': 1,\n",
        "\n",
        "    # Adds a 1x1 convolution directly to the biggest selected layer that predicts a semantic segmentations for each of the 80 classes.\n",
        "    # This branch is only evaluated during training time and is just there for multitask learning.\n",
        "    'use_semantic_segmentation_loss': False,\n",
        "    'semantic_segmentation_alpha': 1,\n",
        "\n",
        "    # Adds another branch to the netwok to predict Mask IoU.\n",
        "    'use_mask_scoring': False,\n",
        "    'mask_scoring_alpha': 1,\n",
        "\n",
        "    # Match gt boxes using the Box2Pix change metric instead of the standard IoU metric.\n",
        "    # Note that the threshold you set for iou_threshold should be negative with this setting on.\n",
        "    'use_change_matching': False,\n",
        "\n",
        "    # Uses the same network format as mask_proto_net, except this time it's for adding extra head layers before the final\n",
        "    # prediction in prediction modules. If this is none, no extra layers will be added.\n",
        "    'extra_head_net': None,\n",
        "\n",
        "    # What params should the final head layers have (the ones that predict box, confidence, and mask coeffs)\n",
        "    'head_layer_params': {'kernel_size': 3, 'padding': 1},\n",
        "\n",
        "    # Add extra layers between the backbone and the network heads\n",
        "    # The order is (bbox, conf, mask)\n",
        "    'extra_layers': (0, 0, 0),\n",
        "\n",
        "    # During training, to match detections with gt, first compute the maximum gt IoU for each prior.\n",
        "    # Then, any of those priors whose maximum overlap is over the positive threshold, mark as positive.\n",
        "    # For any priors whose maximum is less than the negative iou threshold, mark them as negative.\n",
        "    # The rest are neutral and not used in calculating the loss.\n",
        "    'positive_iou_threshold': 0.5,\n",
        "    'negative_iou_threshold': 0.5,\n",
        "\n",
        "    # When using ohem, the ratio between positives and negatives (3 means 3 negatives to 1 positive)\n",
        "    'ohem_negpos_ratio': 3,\n",
        "\n",
        "    # If less than 1, anchors treated as a negative that have a crowd iou over this threshold with\n",
        "    # the crowd boxes will be treated as a neutral.\n",
        "    'crowd_iou_threshold': 1,\n",
        "\n",
        "    # This is filled in at runtime by Yolact's __init__, so don't touch it\n",
        "    'mask_dim': None,\n",
        "\n",
        "    # Input image size.\n",
        "    'max_size': 300,\n",
        "    \n",
        "    # Whether or not to do post processing on the cpu at test time\n",
        "    'force_cpu_nms': True,\n",
        "\n",
        "    # Whether to use mask coefficient cosine similarity nms instead of bbox iou nms\n",
        "    'use_coeff_nms': False,\n",
        "\n",
        "    # Whether or not to have a separate branch whose sole purpose is to act as the coefficients for coeff_diversity_loss\n",
        "    # Remember to turn on coeff_diversity_loss, or these extra coefficients won't do anything!\n",
        "    # To see their effect, also remember to turn on use_coeff_nms.\n",
        "    'use_instance_coeff': False,\n",
        "    'num_instance_coeffs': 64,\n",
        "\n",
        "    # Whether or not to tie the mask loss / box loss to 0\n",
        "    'train_masks': True,\n",
        "    'train_boxes': True,\n",
        "    # If enabled, the gt masks will be cropped using the gt bboxes instead of the predicted ones.\n",
        "    # This speeds up training time considerably but results in much worse mAP at test time.\n",
        "    'use_gt_bboxes': False,\n",
        "\n",
        "    # Whether or not to preserve aspect ratio when resizing the image.\n",
        "    # If True, this will resize all images to be max_size^2 pixels in area while keeping aspect ratio.\n",
        "    # If False, all images are resized to max_size x max_size\n",
        "    'preserve_aspect_ratio': False,\n",
        "\n",
        "    # Whether or not to use the prediction module (c) from DSSD\n",
        "    'use_prediction_module': False,\n",
        "\n",
        "    # Whether or not to use the predicted coordinate scheme from Yolo v2\n",
        "    'use_yolo_regressors': False,\n",
        "    \n",
        "    # For training, bboxes are considered \"positive\" if their anchors have a 0.5 IoU overlap\n",
        "    # or greater with a ground truth box. If this is true, instead of using the anchor boxes\n",
        "    # for this IoU computation, the matching function will use the predicted bbox coordinates.\n",
        "    # Don't turn this on if you're not using yolo regressors!\n",
        "    'use_prediction_matching': False,\n",
        "\n",
        "    # A list of settings to apply after the specified iteration. Each element of the list should look like\n",
        "    # (iteration, config_dict) where config_dict is a dictionary you'd pass into a config object's init.\n",
        "    'delayed_settings': [],\n",
        "\n",
        "    # Use command-line arguments to set this.\n",
        "    'no_jit': False,\n",
        "\n",
        "    'backbone': None,\n",
        "    'name': 'base_config',\n",
        "\n",
        "    # Fast Mask Re-scoring Network\n",
        "    # Inspried by Mask Scoring R-CNN (https://arxiv.org/abs/1903.00241)\n",
        "    # Do not crop out the mask with bbox but slide a convnet on the image-size mask,\n",
        "    # then use global pooling to get the final mask score\n",
        "    'use_maskiou': False,\n",
        "    \n",
        "    # Archecture for the mask iou network. A (num_classes-1, 1, {}) layer is appended to the end.\n",
        "    'maskiou_net': [],\n",
        "\n",
        "    # Discard predicted masks whose area is less than this\n",
        "    'discard_mask_area': -1,\n",
        "\n",
        "    'maskiou_alpha': 1.0,\n",
        "    'rescore_mask': False,\n",
        "    'rescore_bbox': False,\n",
        "    'maskious_to_train': -1,\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- YOLACT v1.0 CONFIGS ----------------------- #\n",
        "\n",
        "yolact_base_config = coco_base_config.copy({\n",
        "    'name': 'yolact_base',\n",
        "\n",
        "    # Dataset stuff\n",
        "    'dataset': my_custom_dataset,\n",
        "    'num_classes': len(my_custom_dataset.class_names) + 1,\n",
        "\n",
        "    # Image Size\n",
        "    'max_size': 550,\n",
        "    \n",
        "    # Training params\n",
        "    'lr_steps': (280000, 600000, 700000, 750000),\n",
        "    'max_iter': 800000,\n",
        "    \n",
        "    # Backbone Settings\n",
        "    'backbone': resnet101_backbone.copy({\n",
        "        'selected_layers': list(range(1, 4)),\n",
        "        'use_pixel_scales': True,\n",
        "        'preapply_sqrt': False,\n",
        "        'use_square_anchors': True, # This is for backward compatability with a bug\n",
        "\n",
        "        'pred_aspect_ratios': [ [[1, 1/2, 2]] ]*5,\n",
        "        'pred_scales': [[24], [48], [96], [192], [384]],\n",
        "    }),\n",
        "\n",
        "    # FPN Settings\n",
        "    'fpn': fpn_base.copy({\n",
        "        'use_conv_downsample': True,\n",
        "        'num_downsample': 2,\n",
        "    }),\n",
        "\n",
        "    # Mask Settings\n",
        "    'mask_type': mask_type.lincomb,\n",
        "    'mask_alpha': 6.125,\n",
        "    'mask_proto_src': 0,\n",
        "    'mask_proto_net': [(256, 3, {'padding': 1})] * 3 + [(None, -2, {}), (256, 3, {'padding': 1})] + [(32, 1, {})],\n",
        "    'mask_proto_normalize_emulate_roi_pooling': True,\n",
        "\n",
        "    # Other stuff\n",
        "    'share_prediction_module': True,\n",
        "    'extra_head_net': [(256, 3, {'padding': 1})],\n",
        "\n",
        "    'positive_iou_threshold': 0.5,\n",
        "    'negative_iou_threshold': 0.4,\n",
        "\n",
        "    'crowd_iou_threshold': 0.7,\n",
        "\n",
        "    'use_semantic_segmentation_loss': True,\n",
        "})\n",
        "\n",
        "yolact_im400_config = yolact_base_config.copy({\n",
        "    'name': 'yolact_im400',\n",
        "\n",
        "    'max_size': 400,\n",
        "    'backbone': yolact_base_config.backbone.copy({\n",
        "        'pred_scales': [[int(x[0] / yolact_base_config.max_size * 400)] for x in yolact_base_config.backbone.pred_scales],\n",
        "    }),\n",
        "})\n",
        "\n",
        "yolact_im700_config = yolact_base_config.copy({\n",
        "    'name': 'yolact_im700',\n",
        "\n",
        "    'masks_to_train': 300,\n",
        "    'max_size': 700,\n",
        "    'backbone': yolact_base_config.backbone.copy({\n",
        "        'pred_scales': [[int(x[0] / yolact_base_config.max_size * 700)] for x in yolact_base_config.backbone.pred_scales],\n",
        "    }),\n",
        "})\n",
        "\n",
        "yolact_darknet53_config = yolact_base_config.copy({\n",
        "    'name': 'yolact_darknet53',\n",
        "\n",
        "    'backbone': darknet53_backbone.copy({\n",
        "        'selected_layers': list(range(2, 5)),\n",
        "        \n",
        "        'pred_scales': yolact_base_config.backbone.pred_scales,\n",
        "        'pred_aspect_ratios': yolact_base_config.backbone.pred_aspect_ratios,\n",
        "        'use_pixel_scales': True,\n",
        "        'preapply_sqrt': False,\n",
        "        'use_square_anchors': True, # This is for backward compatability with a bug\n",
        "    }),\n",
        "})\n",
        "\n",
        "yolact_resnet50_config = yolact_base_config.copy({\n",
        "    'name': 'yolact_resnet50',\n",
        "\n",
        "    'backbone': resnet50_backbone.copy({\n",
        "        'selected_layers': list(range(1, 4)),\n",
        "        \n",
        "        'pred_scales': yolact_base_config.backbone.pred_scales,\n",
        "        'pred_aspect_ratios': yolact_base_config.backbone.pred_aspect_ratios,\n",
        "        'use_pixel_scales': True,\n",
        "        'preapply_sqrt': False,\n",
        "        'use_square_anchors': True, # This is for backward compatability with a bug\n",
        "    }),\n",
        "})\n",
        "\n",
        "\n",
        "yolact_resnet50_pascal_config = yolact_resnet50_config.copy({\n",
        "    'name': None, # Will default to yolact_resnet50_pascal\n",
        "    \n",
        "    # Dataset stuff\n",
        "    'dataset': pascal_sbd_dataset,\n",
        "    'num_classes': len(pascal_sbd_dataset.class_names) + 1,\n",
        "\n",
        "    'max_iter': 120000,\n",
        "    'lr_steps': (60000, 100000),\n",
        "    \n",
        "    'backbone': yolact_resnet50_config.backbone.copy({\n",
        "        'pred_scales': [[32], [64], [128], [256], [512]],\n",
        "        'use_square_anchors': False,\n",
        "    })\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "yolact_resnet50_cornea_config = yolact_resnet50_config.copy({\n",
        "    'name': 'yolact_resnet50_cornea',\n",
        "\n",
        "    # Dataset stuff\n",
        "    'dataset': my_custom_dataset,\n",
        "    'num_classes': len(my_custom_dataset.class_names) + 1,\n",
        "\n",
        "    # Image Size\n",
        "    'max_size': 640,\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- YOLACT++ CONFIGS ----------------------- #\n",
        "\n",
        "yolact_plus_base_config = yolact_base_config.copy({\n",
        "    'name': 'yolact_plus_base',\n",
        "\n",
        "    'backbone': resnet101_dcn_inter3_backbone.copy({\n",
        "        'selected_layers': list(range(1, 4)),\n",
        "        \n",
        "        'pred_aspect_ratios': [ [[1, 1/2, 2]] ]*5,\n",
        "        'pred_scales': [[i * 2 ** (j / 3.0) for j in range(3)] for i in [24, 48, 96, 192, 384]],\n",
        "        'use_pixel_scales': True,\n",
        "        'preapply_sqrt': False,\n",
        "        'use_square_anchors': False,\n",
        "    }),\n",
        "\n",
        "    'use_maskiou': True,\n",
        "    'maskiou_net': [(8, 3, {'stride': 2}), (16, 3, {'stride': 2}), (32, 3, {'stride': 2}), (64, 3, {'stride': 2}), (128, 3, {'stride': 2})],\n",
        "    'maskiou_alpha': 25,\n",
        "    'rescore_bbox': False,\n",
        "    'rescore_mask': True,\n",
        "\n",
        "    'discard_mask_area': 5*5,\n",
        "})\n",
        "\n",
        "yolact_plus_resnet50_config = yolact_plus_base_config.copy({\n",
        "    'name': 'yolact_plus_resnet50',\n",
        "\n",
        "    'backbone': resnet50_dcnv2_backbone.copy({\n",
        "        'selected_layers': list(range(1, 4)),\n",
        "        \n",
        "        'pred_aspect_ratios': [ [[1, 1/2, 2]] ]*5,\n",
        "        'pred_scales': [[i * 2 ** (j / 3.0) for j in range(3)] for i in [24, 48, 96, 192, 384]],\n",
        "        'use_pixel_scales': True,\n",
        "        'preapply_sqrt': False,\n",
        "        'use_square_anchors': False,\n",
        "    }),\n",
        "})\n",
        "\n",
        "\n",
        "# Default config\n",
        "cfg = yolact_base_config.copy()\n",
        "\n",
        "def set_cfg(config_name:str):\n",
        "    \"\"\" Sets the active config. Works even if cfg is already imported! \"\"\"\n",
        "    global cfg\n",
        "\n",
        "    # Note this is not just an eval because I'm lazy, but also because it can\n",
        "    # be used like ssd300_config.copy({'max_size': 400}) for extreme fine-tuning\n",
        "    cfg.replace(eval(config_name))\n",
        "\n",
        "    if cfg.name is None:\n",
        "        cfg.name = config_name.split('_config')[0]\n",
        "\n",
        "def set_dataset(dataset_name:str):\n",
        "    \"\"\" Sets the dataset of the current config. \"\"\"\n",
        "    cfg.dataset = eval(dataset_name)\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRH591_0Vd7W",
        "outputId": "f8c6b26c-a56b-485b-829f-b02a99af6de2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/yolact/data/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Training**"
      ],
      "metadata": {
        "id": "RKFvAfpvW3w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolact\n",
        "!python train.py --config=yolact_resnet50_cornea_config\\\n",
        "                 --resume weights/yolact_base_54_800000.pth\\\n",
        "                 --start_iter 0\\\n",
        "                 --batch_size=8\\\n",
        "                 --save_folder=weights/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv-pOg9PVE7w",
        "outputId": "2a856d1c-1440-4234-e993-27652b123fc0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolact\n",
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Resuming training, loading weights/yolact_base_54_800000.pth...\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 504, in <module>\n",
            "    train()\n",
            "  File \"train.py\", line 207, in train\n",
            "    yolact_net.load_weights(args.resume)\n",
            "  File \"/content/yolact/yolact.py\", line 490, in load_weights\n",
            "    self.load_state_dict(state_dict)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 829, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for Yolact:\n",
            "\tUnexpected key(s) in state_dict: \"backbone.layers.2.6.conv1.weight\", \"backbone.layers.2.6.bn1.weight\", \"backbone.layers.2.6.bn1.bias\", \"backbone.layers.2.6.bn1.running_mean\", \"backbone.layers.2.6.bn1.running_var\", \"backbone.layers.2.6.bn1.num_batches_tracked\", \"backbone.layers.2.6.conv2.weight\", \"backbone.layers.2.6.bn2.weight\", \"backbone.layers.2.6.bn2.bias\", \"backbone.layers.2.6.bn2.running_mean\", \"backbone.layers.2.6.bn2.running_var\", \"backbone.layers.2.6.bn2.num_batches_tracked\", \"backbone.layers.2.6.conv3.weight\", \"backbone.layers.2.6.bn3.weight\", \"backbone.layers.2.6.bn3.bias\", \"backbone.layers.2.6.bn3.running_mean\", \"backbone.layers.2.6.bn3.running_var\", \"backbone.layers.2.6.bn3.num_batches_tracked\", \"backbone.layers.2.7.conv1.weight\", \"backbone.layers.2.7.bn1.weight\", \"backbone.layers.2.7.bn1.bias\", \"backbone.layers.2.7.bn1.running_mean\", \"backbone.layers.2.7.bn1.running_var\", \"backbone.layers.2.7.bn1.num_batches_tracked\", \"backbone.layers.2.7.conv2.weight\", \"backbone.layers.2.7.bn2.weight\", \"backbone.layers.2.7.bn2.bias\", \"backbone.layers.2.7.bn2.running_mean\", \"backbone.layers.2.7.bn2.running_var\", \"backbone.layers.2.7.bn2.num_batches_tracked\", \"backbone.layers.2.7.conv3.weight\", \"backbone.layers.2.7.bn3.weight\", \"backbone.layers.2.7.bn3.bias\", \"backbone.layers.2.7.bn3.running_mean\", \"backbone.layers.2.7.bn3.running_var\", \"backbone.layers.2.7.bn3.num_batches_tracked\", \"backbone.layers.2.8.conv1.weight\", \"backbone.layers.2.8.bn1.weight\", \"backbone.layers.2.8.bn1.bias\", \"backbone.layers.2.8.bn1.running_mean\", \"backbone.layers.2.8.bn1.running_var\", \"backbone.layers.2.8.bn1.num_batches_tracked\", \"backbone.layers.2.8.conv2.weight\", \"backbone.layers.2.8.bn2.weight\", \"backbone.layers.2.8.bn2.bias\", \"backbone.layers.2.8.bn2.running_mean\", \"backbone.layers.2.8.bn2.running_var\", \"backbone.layers.2.8.bn2.num_batches_tracked\", \"backbone.layers.2.8.conv3.weight\", \"backbone.layers.2.8.bn3.weight\", \"backbone.layers.2.8.bn3.bias\", \"backbone.layers.2.8.bn3.running_mean\", \"backbone.layers.2.8.bn3.running_var\", \"backbone.layers.2.8.bn3.num_batches_tracked\", \"backbone.layers.2.9.conv1.weight\", \"backbone.layers.2.9.bn1.weight\", \"backbone.layers.2.9.bn1.bias\", \"backbone.layers.2.9.bn1.running_mean\", \"backbone.layers.2.9.bn1.running_var\", \"backbone.layers.2.9.bn1.num_batches_tracked\", \"backbone.layers.2.9.conv2.weight\", \"backbone.layers.2.9.bn2.weight\", \"backbone.layers.2.9.bn2.bias\", \"backbone.layers.2.9.bn2.running_mean\", \"backbone.layers.2.9.bn2.running_var\", \"backbone.layers.2.9.bn2.num_batches_tracked\", \"backbone.layers.2.9.conv3.weight\", \"backbone.layers.2.9.bn3.weight\", \"backbone.layers.2.9.bn3.bias\", \"backbone.layers.2.9.bn3.running_mean\", \"backbone.layers.2.9.bn3.running_var\", \"backbone.layers.2.9.bn3.num_batches_tracked\", \"backbone.layers.2.10.conv1.weight\", \"backbone.layers.2.10.bn1.weight\", \"backbone.layers.2.10.bn1.bias\", \"backbone.layers.2.10.bn1.running_mean\", \"backbone.layers.2.10.bn1.running_var\", \"backbone.layers.2.10.bn1.num_batches_tracked\", \"backbone.layers.2.10.conv2.weight\", \"backbone.layers.2.10.bn2.weight\", \"backbone.layers.2.10.bn2.bias\", \"backbone.layers.2.10.bn2.running_mean\", \"backbone.layers.2.10.bn2.running_var\", \"backbone.layers.2.10.bn2.num_batches_tracked\", \"backbone.layers.2.10.conv3.weight\", \"backbone.layers.2.10.bn3.weight\", \"backbone.layers.2.10.bn3.bias\", \"backbone.layers.2.10.bn3.running_mean\", \"backbone.layers.2.10.bn3.running_var\", \"backbone.layers.2.10.bn3.num_batches_tracked\", \"backbone.layers.2.11.conv1.weight\", \"backbone.layers.2.11.bn1.weight\", \"backbone.layers.2.11.bn1.bias\", \"backbone.layers.2.11.bn1.running_mean\", \"backbone.layers.2.11.bn1.running_var\", \"backbone.layers.2.11.bn1.num_batches_tracked\", \"backbone.layers.2.11.conv2.weight\", \"backbone.layers.2.11.bn2.weight\", \"backbone.layers.2.11.bn2.bias\", \"backbone.layers.2.11.bn2.running_mean\", \"backbone.layers.2.11.bn2.running_var\", \"backbone.layers.2.11.bn2.num_batches_tracked\", \"backbone.layers.2.11.conv3.weight\", \"backbone.layers.2.11.bn3.weight\", \"backbone.layers.2.11.bn3.bias\", \"backbone.layers.2.11.bn3.running_mean\", \"backbone.layers.2.11.bn3.running_var\", \"backbone.layers.2.11.bn3.num_batches_tracked\", \"backbone.layers.2.12.conv1.weight\", \"backbone.layers.2.12.bn1.weight\", \"backbone.layers.2.12.bn1.bias\", \"backbone.layers.2.12.bn1.running_mean\", \"backbone.layers.2.12.bn1.running_var\", \"backbone.layers.2.12.bn1.num_batches_tracked\", \"backbone.layers.2.12.conv2.weight\", \"backbone.layers.2.12.bn2.weight\", \"backbone.layers.2.12.bn2.bias\", \"backbone.layers.2.12.bn2.running_mean\", \"backbone.layers.2.12.bn2.running_var\", \"backbone.layers.2.12.bn2.num_batches_tracked\", \"backbone.layers.2.12.conv3.weight\", \"backbone.layers.2.12.bn3.weight\", \"backbone.layers.2.12.bn3.bias\", \"backbone.layers.2.12.bn3.running_mean\", \"backbone.layers.2.12.bn3.running_var\", \"backbone.layers.2.12.bn3.num_batches_tracked\", \"backbone.layers.2.13.conv1.weight\", \"backbone.layers.2.13.bn1.weight\", \"backbone.layers.2.13.bn1.bias\", \"backbone.layers.2.13.bn1.running_mean\", \"backbone.layers.2.13.bn1.running_var\", \"backbone.layers.2.13.bn1.num_batches_tracked\", \"backbone.layers.2.13.conv2.weight\", \"backbone.layers.2.13.bn2.weight\", \"backbone.layers.2.13.bn2.bias\", \"backbone.layers.2.13.bn2.running_mean\", \"backbone.layers.2.13.bn2.running_var\", \"backbone.layers.2.13.bn2.num_batches_tracked\", \"backbone.layers.2.13.conv3.weight\", \"backbone.layers.2.13.bn3.weight\", \"backbone.layers.2.13.bn3.bias\", \"backbone.layers.2.13.bn3.running_mean\", \"backbone.layers.2.13.bn3.running_var\", \"backbone.layers.2.13.bn3.num_batches_tracked\", \"backbone.layers.2.14.conv1.weight\", \"backbone.layers.2.14.bn1.weight\", \"backbone.layers.2.14.bn1.bias\", \"backbone.layers.2.14.bn1.running_mean\", \"backbone.layers.2.14.bn1.running_var\", \"backbone.layers.2.14.bn1.num_batches_tracked\", \"backbone.layers.2.14.conv2.weight\", \"backbone.layers.2.14.bn2.weight\", \"backbone.layers.2.14.bn2.bias\", \"backbone.layers.2.14.bn2.running_mean\", \"backbone.layers.2.14.bn2.running_var\", \"backbone.layers.2.14.bn2.num_batches_tracked\", \"backbone.layers.2.14.conv3.weight\", \"backbone.layers.2.14.bn3.weight\", \"backbone.layers.2.14.bn3.bias\", \"backbone.layers.2.14.bn3.running_mean\", \"backbone.layers.2.14.bn3.running_var\", \"backbone.layers.2.14.bn3.num_batches_tracked\", \"backbone.layers.2.15.conv1.weight\", \"backbone.layers.2.15.bn1.weight\", \"backbone.layers.2.15.bn1.bias\", \"backbone.layers.2.15.bn1.running_mean\", \"backbone.layers.2.15.bn1.running_var\", \"backbone.layers.2.15.bn1.num_batches_tracked\", \"backbone.layers.2.15.conv2.weight\", \"backbone.layers.2.15.bn2.weight\", \"backbone.layers.2.15.bn2.bias\", \"backbone.layers.2.15.bn2.running_mean\", \"backbone.layers.2.15.bn2.running_var\", \"backbone.layers.2.15.bn2.num_batches_tracked\", \"backbone.layers.2.15.conv3.weight\", \"backbone.layers.2.15.bn3.weight\", \"backbone.layers.2.15.bn3.bias\", \"backbone.layers.2.15.bn3.running_mean\", \"backbone.layers.2.15.bn3.running_var\", \"backbone.layers.2.15.bn3.num_batches_tracked\", \"backbone.layers.2.16.conv1.weight\", \"backbone.layers.2.16.bn1.weight\", \"backbone.layers.2.16.bn1.bias\", \"backbone.layers.2.16.bn1.running_mean\", \"backbone.layers.2.16.bn1.running_var\", \"backbone.layers.2.16.bn1.num_batches_tracked\", \"backbone.layers.2.16.conv2.weight\", \"backbone.layers.2.16.bn2.weight\", \"backbone.layers.2.16.bn2.bias\", \"backbone.layers.2.16.bn2.running_mean\", \"backbone.layers.2.16.bn2.running_var\", \"backbone.layers.2.16.bn2.num_batches_tracked\", \"backbone.layers.2.16.conv3.weight\", \"backbone.layers.2.16.bn3.weight\", \"backbone.layers.2.16.bn3.bias\", \"backbone.layers.2.16.bn3.running_mean\", \"backbone.layers.2.16.bn3.running_var\", \"backbone.layers.2.16.bn3.num_batches_tracked\", \"backbone.layers.2.17.conv1.weight\", \"backbone.layers.2.17.bn1.weight\", \"backbone.layers.2.17.bn1.bias\", \"backbone.layers.2.17.bn1.running_mean\", \"backbone.layers.2.17.bn1.running_var\", \"backbone.layers.2.17.bn1.num_batches_tracked\", \"backbone.layers.2.17.conv2.weight\", \"backbone.layers.2.17.bn2.weight\", \"backbone.layers.2.17.bn2.bias\", \"backbone.layers.2.17.bn2.running_mean\", \"backbone.layers.2.17.bn2.running_var\", \"backbone.layers.2.17.bn2.num_batches_tracked\", \"backbone.layers.2.17.conv3.weight\", \"backbone.layers.2.17.bn3.weight\", \"backbone.layers.2.17.bn3.bias\", \"backbone.layers.2.17.bn3.running_mean\", \"backbone.layers.2.17.bn3.running_var\", \"backbone.layers.2.17.bn3.num_batches_tracked\", \"backbone.layers.2.18.conv1.weight\", \"backbone.layers.2.18.bn1.weight\", \"backbone.layers.2.18.bn1.bias\", \"backbone.layers.2.18.bn1.running_mean\", \"backbone.layers.2.18.bn1.running_var\", \"backbone.layers.2.18.bn1.num_batches_tracked\", \"backbone.layers.2.18.conv2.weight\", \"backbone.layers.2.18.bn2.weight\", \"backbone.layers.2.18.bn2.bias\", \"backbone.layers.2.18.bn2.running_mean\", \"backbone.layers.2.18.bn2.running_var\", \"backbone.layers.2.18.bn2.num_batches_tracked\", \"backbone.layers.2.18.conv3.weight\", \"backbone.layers.2.18.bn3.weight\", \"backbone.layers.2.18.bn3.bias\", \"backbone.layers.2.18.bn3.running_mean\", \"backbone.layers.2.18.bn3.running_var\", \"backbone.layers.2.18.bn3.num_batches_tracked\", \"backbone.layers.2.19.conv1.weight\", \"backbone.layers.2.19.bn1.weight\", \"backbone.layers.2.19.bn1.bias\", \"backbone.layers.2.19.bn1.running_mean\", \"backbone.layers.2.19.bn1.running_var\", \"backbone.layers.2.19.bn1.num_batches_tracked\", \"backbone.layers.2.19.conv2.weight\", \"backbone.layers.2.19.bn2.weight\", \"backbone.layers.2.19.bn2.bias\", \"backbone.layers.2.19.bn2.running_mean\", \"backbone.layers.2.19.bn2.running_var\", \"backbone.layers.2.19.bn2.num_batches_tracked\", \"backbone.layers.2.19.conv3.weight\", \"backbone.layers.2.19.bn3.weight\", \"backbone.layers.2.19.bn3.bias\", \"backbone.layers.2.19.bn3.running_mean\", \"backbone.layers.2.19.bn3.running_var\", \"backbone.layers.2.19.bn3.num_batches_tracked\", \"backbone.layers.2.20.conv1.weight\", \"backbone.layers.2.20.bn1.weight\", \"backbone.layers.2.20.bn1.bias\", \"backbone.layers.2.20.bn1.running_mean\", \"backbone.layers.2.20.bn1.running_var\", \"backbone.layers.2.20.bn1.num_batches_tracked\", \"backbone.layers.2.20.conv2.weight\", \"backbone.layers.2.20.bn2.weight\", \"backbone.layers.2.20.bn2.bias\", \"backbone.layers.2.20.bn2.running_mean\", \"backbone.layers.2.20.bn2.running_var\", \"backbone.layers.2.20.bn2.num_batches_tracked\", \"backbone.layers.2.20.conv3.weight\", \"backbone.layers.2.20.bn3.weight\", \"backbone.layers.2.20.bn3.bias\", \"backbone.layers.2.20.bn3.running_mean\", \"backbone.layers.2.20.bn3.running_var\", \"backbone.layers.2.20.bn3.num_batches_tracked\", \"backbone.layers.2.21.conv1.weight\", \"backbone.layers.2.21.bn1.weight\", \"backbone.layers.2.21.bn1.bias\", \"backbone.layers.2.21.bn1.running_mean\", \"backbone.layers.2.21.bn1.running_var\", \"backbone.layers.2.21.bn1.num_batches_tracked\", \"backbone.layers.2.21.conv2.weight\", \"backbone.layers.2.21.bn2.weight\", \"backbone.layers.2.21.bn2.bias\", \"backbone.layers.2.21.bn2.running_mean\", \"backbone.layers.2.21.bn2.running_var\", \"backbone.layers.2.21.bn2.num_batches_tracked\", \"backbone.layers.2.21.conv3.weight\", \"backbone.layers.2.21.bn3.weight\", \"backbone.layers.2.21.bn3.bias\", \"backbone.layers.2.21.bn3.running_mean\", \"backbone.layers.2.21.bn3.running_var\", \"backbone.layers.2.21.bn3.num_batches_tracked\", \"backbone.layers.2.22.conv1.weight\", \"backbone.layers.2.22.bn1.weight\", \"backbone.layers.2.22.bn1.bias\", \"backbone.layers.2.22.bn1.running_mean\", \"backbone.layers.2.22.bn1.running_var\", \"backbone.layers.2.22.bn1.num_batches_tracked\", \"backbone.layers.2.22.conv2.weight\", \"backbone.layers.2.22.bn2.weight\", \"backbone.layers.2.22.bn2.bias\", \"backbone.layers.2.22.bn2.running_mean\", \"backbone.layers.2.22.bn2.running_var\", \"backbone.layers.2.22.bn2.num_batches_tracked\", \"backbone.layers.2.22.conv3.weight\", \"backbone.layers.2.22.bn3.weight\", \"backbone.layers.2.22.bn3.bias\", \"backbone.layers.2.22.bn3.running_mean\", \"backbone.layers.2.22.bn3.running_var\", \"backbone.layers.2.22.bn3.num_batches_tracked\". \n",
            "\tsize mismatch for prediction_layers.0.conf_layer.weight: copying a param with shape torch.Size([243, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([45, 256, 3, 3]).\n",
            "\tsize mismatch for prediction_layers.0.conf_layer.bias: copying a param with shape torch.Size([243]) from checkpoint, the shape in current model is torch.Size([45]).\n",
            "\tsize mismatch for semantic_seg_conv.weight: copying a param with shape torch.Size([80, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([14, 256, 1, 1]).\n",
            "\tsize mismatch for semantic_seg_conv.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([14]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_path = \"/content/drive/MyDrive/Deep_learning/Congenital_Glaucoma/glaucoma_cornea_ellipse_coco_train/annotations/val.json\""
      ],
      "metadata": {
        "id": "mTnnjLdAHMdr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# COCO JSONファイルを読み込む\n",
        "with open(json_path, \"r\") as f:\n",
        "    coco_json = json.load(f)\n",
        "coco_json[\"categories\"]"
      ],
      "metadata": {
        "id": "ym4bUu9NHOSs",
        "outputId": "b315c7f4-6f6e-4a31-ed35-40e9c2c20efa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 1, 'name': 'corneal margin', 'supercategory': ''}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}